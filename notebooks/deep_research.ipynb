{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RefSlave Agent Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LLM Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def system_prompt() -> str:\n",
    "    \"\"\"현재 타임스탬프를 포함한 시스템 프롬프트를 생성합니다.\"\"\"\n",
    "    now = datetime.now().isoformat()\n",
    "    return f\"\"\"당신은 전문 연구원입니다. 오늘 날짜는 {now}입니다. 응답 시 다음 지침을 따르세요:\n",
    "    - 지식 컷오프 이후의 주제에 대한 조사를 요청받을 수 있습니다. 사용자가 뉴스 내용을 제시했다면, 그것을 사실로 가정하세요.\n",
    "    - 사용자는 매우 숙련된 분석가이므로 내용을 단순화할 필요 없이 가능한 한 자세하고 정확하게 응답하세요.\n",
    "    - 체계적으로 정보를 정리하세요.\n",
    "    - 사용자가 생각하지 못한 해결책을 제안하세요.\n",
    "    - 적극적으로 사용자의 필요를 예측하고 대응하세요.\n",
    "    - 사용자를 모든 분야의 전문가로 대우하세요.\n",
    "    - 실수는 신뢰를 저하시킵니다. 정확하고 철저하게 응답하세요.\n",
    "    - 상세한 설명을 제공하세요. 사용자는 많은 정보를 받아들일 수 있습니다.\n",
    "    - 권위보다 논리적 근거를 우선하세요. 출처 자체는 중요하지 않습니다.\n",
    "    - 기존의 통념뿐만 아니라 최신 기술과 반대 의견도 고려하세요.\n",
    "    - 높은 수준의 추측이나 예측을 포함할 수 있습니다. 단, 이를 명확히 표시하세요.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loder = ArxivLoader(\n",
    "    query=\"Chane of Thought\",\n",
    "    load_max_docs=10,\n",
    "    load_all_available=True\n",
    ")\n",
    "docs = loder.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 key 목록: ['id', 'metadata', 'page_content', 'type']\n",
      "문서의 metadata: {'Published': '2017-01-17', 'Title': 'Slow-light analogue with a ladder of RLC circuits', 'Authors': 'J. -P. Cromières, T. Chanelière', 'Summary': 'The linear susceptibility of an atomic sample is formally equivalent to the\\nresponse of a RLC circuit. We use a ladder of lumped RLC circuits to observe an\\nanalogue of slow-light, a well-known phenomenon in atomic physics. We first\\ncharacterize the radio-frequency response of the circuit in the spectral domain\\nexhibiting a transparency window surrounded by two strongly absorptive lines.\\nWe then observe a delayed pulse whose group delay is comparable to the pulse\\nduration corresponding to slow-light propagation. The large group delay is\\nobtained by cascading in a ladder configuration doubly resonant RLC cells.'}\n",
      "문서의 id: None\n"
     ]
    }
   ],
   "source": [
    "# json 데이터의 key값 확인\n",
    "import json\n",
    "data = docs[0].model_dump_json()\n",
    "keys = json.loads(data).keys()\n",
    "print(\"문서의 key 목록:\", list(keys))\n",
    "print(\"문서의 metadata:\", docs[0].metadata)\n",
    "print(\"문서의 id:\", docs[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    title: str\n",
    "    published: str\n",
    "    summary: str\n",
    "    authors: List[str]\n",
    "    page_content: str\n",
    "\n",
    "\n",
    "def parse_arxiv_result(doc) -> SearchResult:\n",
    "    \"\"\"Arxiv 문서를 SearchResult 객체로 변환하는 함수\"\"\"\n",
    "    return SearchResult(\n",
    "        title=doc.metadata.get('Title', ''),\n",
    "        published=doc.metadata.get('Published', ''),\n",
    "        summary=doc.metadata.get('Summary', ''),\n",
    "        authors=doc.metadata.get('Authors', []).split(','),\n",
    "        page_content=doc.page_content\n",
    "    )\n",
    "\n",
    "\n",
    "def search_arxiv(query: str, max_docs: int = 10) -> List[SearchResult]:\n",
    "    \"\"\"Arxiv 검색 결과를 SearchResult 객체 리스트로 반환하는 함수\"\"\"\n",
    "    loader = ArxivLoader(\n",
    "        query=query,\n",
    "        load_max_docs=max_docs,\n",
    "        load_all_available=True\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    return [parse_arxiv_result(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(title='Contrastive Chain-of-Thought Prompting', published='2023-11-15', summary='Despite the success of chain of thought in enhancing language model\\nreasoning, the underlying process remains less well understood. Although\\nlogically sound reasoning appears inherently crucial for chain of thought,\\nprior studies surprisingly reveal minimal impact when using invalid\\ndemonstrations instead. Furthermore, the conventional chain of thought does not\\ninform language models on what mistakes to avoid, which potentially leads to\\nmore errors. Hence, inspired by how humans can learn from both positive and\\nnegative examples, we propose contrastive chain of thought to enhance language\\nmodel reasoning. Compared to the conventional chain of thought, our approach\\nprovides both valid and invalid reasoning demonstrations, to guide the model to\\nreason step-by-step while reducing reasoning mistakes. To improve\\ngeneralization, we introduce an automatic method to construct contrastive\\ndemonstrations. Our experiments on reasoning benchmarks demonstrate that\\ncontrastive chain of thought can serve as a general enhancement of\\nchain-of-thought prompting.', authors=['Yew Ken Chia', ' Guizhen Chen', ' Luu Anh Tuan', ' Soujanya Poria', ' Lidong Bing'], page_content='Contrastive Chain-of-Thought Prompting\\nYew Ken Chia∗1,\\nGuizhen Chen∗1, 2\\nLuu Anh Tuan2\\nSoujanya Poria\\nLidong Bing† 1\\n1DAMO Academy, Alibaba Group, Singapore\\nSingapore University of Technology and Design\\n2Nanyang Technological University, Singapore\\n{yewken_chia, sporia}@sutd.edu.sg\\n{guizhen001, anhtuan.luu}@ntu.edu.sg\\n{yewken.chia, guizhen.chen, l.bing}@alibaba-inc.com\\nAbstract\\nDespite the success of chain of thought in en-\\nhancing language model reasoning, the under-\\nlying process remains less well understood. Al-\\nthough logically sound reasoning appears in-\\nherently crucial for chain of thought, prior stud-\\nies surprisingly reveal minimal impact when\\nusing invalid demonstrations instead. Further-\\nmore, the conventional chain of thought does\\nnot inform language models on what mistakes\\nto avoid, which potentially leads to more er-\\nrors.\\nHence, inspired by how humans can\\nlearn from both positive and negative exam-\\nples, we propose contrastive chain of thought\\nto enhance language model reasoning. Com-\\npared to the conventional chain of thought, our\\napproach provides both valid and invalid rea-\\nsoning demonstrations, to guide the model to\\nreason step-by-step while reducing reasoning\\nmistakes. To improve generalization, we in-\\ntroduce an automatic method to construct con-\\ntrastive demonstrations. Our experiments on\\nreasoning benchmarks demonstrate that con-\\ntrastive chain of thought can serve as a general\\nenhancement of chain-of-thought prompting.1\\n1\\nIntroduction\\nWith the trend of large language models (LLMs),\\nmassively scaling the model size has enabled\\ngreater generalization (Brown et al., 2020) and\\nthe emergent ability to perform new tasks when\\ngiven suitable prompts (Wei et al., 2022a). How-\\never, solely increasing the model size cannot solve\\ncomplex reasoning tasks (Rae et al., 2022). To this\\nend, chain-of-thought prompting was proposed to\\nunlock the reasoning ability of LLMs by generating\\nintermediate reasoning steps (Wei et al., 2022b). In\\n∗Equal contribution. Yew Ken and Guizhen are students\\nunder the Joint PhD Program between Alibaba and their cor-\\nresponding university.\\n†Corresponding author.\\n1Our\\ncode\\nimplementation\\nwill\\nbe\\nreleased\\nat\\nhttps://github.com/DAMO-NLP-SG/contrastive-cot\\nExplanation: The dentist drills 4 teeth, so there are 30 - \\n4 = 26 teeth left. The dentist caps 7 more teeth than he \\ndrills, so he caps 4 + 7 = 11 teeth. Therefore, the \\ndentist fixes a total of 4 + 11 = 15 teeth. To find the \\npercentage of teeth the dentist fixes, we divide the \\nnumber of teeth fixed by the total number of teeth and \\nmultiply by 100: 15/30 x 100 = 50%\\nQuestion : James writes a 3-page letter to 2 different \\nfriends twice a week. How many pages does he write \\na year?\\nExplanation: He writes each friend 3*2=6 pages a \\nweek. So he writes 6*2=12 pages every week. That \\nmeans he writes 12*52=624 pages a year.\\nWrong Explanation: He writes each friend 12*52=624 \\npages a week. So he writes 3*2=6 pages every week. \\nThat means he writes 6*2=12 pages a year.\\nQuestion: James has 30 teeth. His dentist drills 4 of \\nthem and caps 7 more teeth than he drills. What \\npercentage of James\\' teeth does the dentist fix?\\nModel Input\\nModel Output\\nFigure 1: Example of contrastive chain-of-thought\\nwhich leverages both positive and negative demonstra-\\ntions to enhance language model reasoning.\\npractice, most methods based on chain of thought\\nleverage in-context learning (Brown et al., 2020)by\\nprompting the model with demonstrations of the in-\\nput, chain-of-thought, and output (Chu et al., 2023).\\nHowever, despite its success, we lack a thor-\\nough understanding of the chain of thought (Cooper\\net al., 2021). For example, it was shown that even\\ndemonstrations with invalid reasoning can lead to\\nsimilar performance compared to valid demonstra-\\ntions (Wang et al., 2023)2. Hence, it is not clear\\nhow language models learn to reason effectively\\nbased on the chain-of-thought demonstrations. On\\nthe other hand, mistakes in the intermediate steps\\ncan compound and derail the reasoning process\\n2Note that while chain-of-thought can be performed in\\na zero-shot fashion with prompts, we focus on the few-shot\\nsetting, as it was originally proposed in Wei et al. (2022b).\\narXiv:2311.09277v1  [cs.CL]  15 Nov 2023\\n(Ling et al., 2023). Any potential error in the rea-\\nsoning process not only affects the accuracy of the\\nfinal result but also undermines the trustworthiness\\nof the language model (Turpin et al., 2023). Thus,\\nit is also important to reduce mistakes in intermedi-\\nate reasoning steps.\\nTo address the challenges of chain of thought,\\nwe are inspired by how humans can learn from pos-\\nitive as well as negative examples. For instance,\\nwhen solving a complex task where the intermedi-\\nate steps are not well-defined, it is useful to learn\\nthe correct steps from positive demonstrations, as\\nwell as avoiding faults in negative demonstrations.\\nHence, we propose contrastive chain of thought,\\nwhich provides both positive and negative demon-\\nstrations to enhance the reasoning of language mod-\\nels. Naturally, this raises the question of how to\\ndesign effective negative demonstrations, as well\\nas whether they can be generalized to diverse tasks.\\nThrough our analysis of multiple invalid reasoning\\ntypes, we design a simple and effective method\\nthat can automatically generate contrastive demon-\\nstrations from existing valid reasoning chains. Fur-\\nthermore, as contrastive chain-of-thought is task-\\nagnostic and compatible with methods such as self-\\nconsistency (Wang et al., 2022), we believe that\\nit can serve as a general enhancement of chain of\\nthought.\\nTo measure the effectiveness of contrastive chain\\nof thought, we present evaluations on a wide range\\nof reasoning benchmarks, and find significant ben-\\nefits. Notably, compared to conventional chain of\\nthought, we observe improvements of 9.8 and 16.0\\npoints for GSM-8K (Cobbe et al., 2021) and Bam-\\nboogle (Press et al., 2023) respectively when using\\nGPT-3.5-Turbo3, a widely used LLM. Further anal-\\nysis of the reasoning chains generated from our\\nmethod also shows significant reduction in errors.\\nIn summary, our main contributions include: (1)\\nWe analyse various invalid reasoning types and\\nfind that combining positive and negative demon-\\nstrations generally boost the effectiveness of chain-\\nof-thought. (2) Based on the analysis above, we\\npropose contrastive chain of thought to enhance lan-\\nguage model reasoning. To improve generalization,\\nwe also propose an automatic method to construct\\ncontrastive demonstrations. (3) Evaluations on mul-\\ntiple reasoning benchmarks demonstrate significant\\nimprovements compared to conventional chain of\\nthought.\\n3https://platform.openai.com/docs/models\\n2\\nPreliminary Study: Effect of Different\\nTypes of Contrastive Demonstrations\\nWhile chain of thought (CoT) prompting has en-\\nhanced the reasoning of large language models, it\\nremains less well understood. For instance, while\\nsound reasoning seems intuitively important to ef-\\nfective chain of thought, previous work has shown\\nthat there is little effect when using invalid demon-\\nstrations. On the other hand, previous works in\\ncontrastive learning (Khosla et al., 2020) and align-\\nment (Ouyang et al., 2022) have demonstrated how\\nlanguage models can learn more effectively from\\nboth valid and invalid examples. Hence, we con-\\nduct a preliminary study with the following re-\\nsearch question: Can invalid reasoning demon-\\nstrations be instead used to enhance chain of\\nthought? Specifically, we aim to study the effect\\nof providing chain-of-thought demonstrations in a\\n“contrastive” manner, i.e., demonstrations contain-\\ning both valid and invalid rationales.\\n2.1\\nComponents of Chain of Thought\\nCompared to standard prompting with in-context\\ndemonstrations (Brown et al., 2020), chain-of-\\nthought (CoT) prompting (Wei et al., 2022b) in-\\ncludes a rationale for each demonstration example.\\nEach rationale consists of a series of intermedi-\\nate reasoning steps, guiding the language model to\\nsolve tasks in a step-by-step manner. Following the\\nformulation of (Wang et al., 2023), we identify two\\ndistinct components of each CoT rationale:\\n• Bridging objects are the symbolic items that\\nthe model traverses in order to reach the final\\nsolution. For example, the objects could be\\nnumbers and equations in arithmetic tasks, or\\nthe names of entities in factual tasks.\\n• Language templates are the textual hints that\\nguide the language model to derive and con-\\ntextualize the correct bridging objects during\\nthe reasoning process.\\n2.2\\nWhat is Invalid Chain of Thought?\\nGiven the distinct components of chain of thought,\\nwe are now able to systematically identify the as-\\npects which lead to invalid rationales. Concretely\\nthere are two main aspects which are applicable to\\nboth the language and object components:\\n• Coherence refers to the correct ordering of\\nsteps in a rationale, and is necessary for suc-\\ncessful chain of thought. Specifically, as chain\\nPrompting\\nMethod\\nArithmetic Reasoning Example\\nQuestion: Leah had 32 chocolates and her sister had 42. \\nIf they ate 35, how many pieces do they have left in total?\\nFactual Reasoning Example\\nQuestion: Who is the grandchild of Dambar Shah?\\nStandard\\nAnswer: 39\\nAnswer: Rudra Shah\\nChain-of-Thought \\n(CoT)\\nStep-by-step answer: Originally, Leah had 32 chocolates \\nand her sister had 42. So in total they had 32 + 42 = 74. \\nAfter eating 35, they had 74 - 35 = 39 pieces left in total. \\nThe answer is 39.\\nStep-by-step answer: Dambar Shah (? - 1645) was the \\nfather of Krishna Shah. Rudra Shah was the child of \\nKrishna Shah (? - 1661). So the final answer (the name \\nof the grandchild) is: Rudra Shah.\\nCoT: Invalid \\nReasoning\\nStep-by-step answer: Originally, Leah had 32 chocolates \\nand her sister had 42. So her sister had 42 - 32 = 10 \\nchocolates more than Leah has. After eating 35, since 10 \\n+ 35 = 45, they had 45 - 6 = 39 pieces left in total. The \\nanswer is 39. \\nStep-by-step answer: Dambar Shah (? - 1645) was the \\nking of the Gorkha Kingdom. The Gorkha Kingdom was \\nestablished by Prince Dravya Shah. Dravya Shah has a \\nchild named Rudra Shah. So the final answer (the name \\nof the grandchild) is: Rudra Shah.\\nCoT: Incoherent \\nObjects\\nStep-by-step answer: Originally, Leah had 32 + 42 = 74 \\nchocolates and her sister had 32. So in total they had 74 - \\n35 = 39. After eating 35, they had 42 pieces left in total. \\nThe answer is 39.\\nStep-by-step answer: Krishna Shah was the father of \\nRudra Shah. Dambar Shah (? - 1645) was the child of \\nKrishna Shah (? - 1661). So the final answer (the name \\nof the grandchild) is: Rudra Shah.\\nCoT: Incoherent \\nLanguage\\nStep-by-step answer: After eating 32, they had 42 pieces \\nleft in total. Originally, Leah had 32 + 42 = 74 chocolates \\nand her sister had 35. So in total they had 74 - 35 = 39. \\nThe answer is 39.\\nStep-by-step answer: Dambar Shah (? - 1645) was the \\nchild of Krishna Shah. Rudra Shah was the father of \\nKrishna Shah (? - 1661). So the final answer (the name \\nof the grandchild) is: Rudra Shah.\\nCoT: Irrelevant \\nObjects\\nStep-by-step answer: Originally, Leah had 19 chocolates \\nand her sister had 31. So in total they had 19 + 31 = 50. \\nAfter eating 29, they had 50 - 29 = 21 pieces left in total. \\nThe answer is 21.\\nStep-by-step answer: Metis Amando was the father of \\nDavid Amando. Randall Amando was the child of David \\nAmando. So the final answer (the name of the \\ngrandchild) is: Randall Amando.\\nCoT: Irrelevant \\nLanguage\\nStep-by-step answer: Patricia needs to donate 32 inches, \\nand wants her hair to be 42 inches long after the donation. \\nHer hair is 35 inches long currently. Her hair needs to be \\n32 + 42 = 74 inches long when she cuts it. So she needs \\nto grow 74 - 35 = 39 more inches. The answer is 39.\\nStep-by-step answer: The husband of Dambar Shah (? - \\n1645) is Krishna Shah. Krishna Shah (? - 1661) has a \\nbrother called Rudra Shah. So the final answer (the \\nname of the brother-in-law) is: Rudra Shah.\\nLanguage Component\\nObject Component\\nInvalid Component (Reasoning / Language / Object)\\nFigure 2: Categorization of invalid chain-of-thought examples, following Wang et al. (2023).\\nof thought is a sequential reasoning process,\\nit is not possible for later steps to be pre-\\nconditions of earlier steps.\\n• Relevance refers to whether the rationale\\ncontains corresponding information from the\\nquestion. For instance, if the question men-\\ntions a person named Leah eating chocolates,\\nit would be irrelevant to discuss a different\\nperson cutting their hair.\\nIn addition, following Wang et al. (2023), we\\ninclude invalid reasoning as a category of invalid\\nchain of thought, which is neither incoherent nor\\nirrelevant, but contains logical mistakes. Hence,\\nwe aim to study the five main categories of invalid\\nchain-of-thought, as shown in Figure 2.\\n2.3\\nExperimental Setup\\nTo conduct the experiments for the preliminary\\nstudy, we leverage the GSM8K (Cobbe et al., 2021)\\nand Bamboogle (Press et al., 2023) datasets for\\narithmetic and factual reasoning respectively. We\\nuse the OpenAI Chat Completions API4 which is\\none of the most popular and well-performing lan-\\nguage models with reasonable cost. Specifically,\\nwe use the GPT-3.5-Turbo (0301) version. To study\\nthe effect of contrastive demonstrations under vari-\\nous settings, we evaluate the five main invalid cat-\\negories as shown in Figure 2. Note that we use\\n4-shot prompting for each dataset, and the chain-of-\\nthought demonstrations are manually constructed\\nby previous works (Wei et al., 2022b; Wang et al.,\\n2023). To standardize the prompting process, we\\nuse a simplified chain-of-thought prompt format,\\nas shown in Figure 1.\\n2.4\\nPreliminary Results\\nBased on the preliminary results in Table 1, we\\nobserve significant gains across all invalid ratio-\\nnale categories compared to conventional chain-\\nof-thought. Notably, leveraging chain of thought\\nwith contrastive demonstrations containing incoher-\\nent objects yields the highest average performance\\n4https://platform.openai.com/docs/api-reference\\nAnswer: 37.5%\\nExplanation: The dentist fixes a total of \\n4 + 7 = 11 teeth. To find the \\npercentage, we divide the number of \\nteeth fixed by the total number of teeth \\nand multiply by 100: 11/30 x 100 = \\n36.67%\\nExplanation: The dentist drills 4 teeth, so there \\nare 30 - 4 = 26 teeth left. The dentist caps 7 \\nmore teeth than he drills, so he caps 4 + 7 = 11 \\nteeth. Therefore, the dentist fixes a total of 4 + 11 \\n= 15 teeth. To find the percentage of teeth the \\ndentist fixes, we divide the number of teeth fixed \\nby the total number of teeth and multiply by 100: \\n15/30 x 100 = 50%\\nQuestion : James writes a 3-page letter \\nto 2 different friends twice a week. How \\nmany pages does he write a year?\\nExplanation: He writes each friend \\n3*2=6 pages a week So he writes \\n6*2=12 pages every week. That means \\nhe writes 12*52=624 pages a year.\\nQuestion: James has 30 teeth. His \\ndentist drills 4 of them and caps 7 more \\nteeth than he drills. What percentage of \\nJames\\' teeth does the dentist fix?\\nQuestion : James writes a 3-page letter to 2 \\ndifferent friends twice a week. How many pages \\ndoes he write a year?\\nExplanation: He writes each friend 3*2=6 pages \\na week. So he writes 6*2=12 pages every week. \\nThat means he writes 12*52=624 pages a year.\\nWrong Explanation: He writes each friend \\n12*52=624 pages a week. So he writes 3*2=6 \\npages every week. That means he writes 6*2=12 \\npages a year.\\nQuestion: James has 30 teeth. His dentist drills 4 \\nof them and caps 7 more teeth than he drills. \\nWhat percentage of James\\' teeth does the \\ndentist fix?\\nQuestion: James writes a \\n3-page letter to 2 different \\nfriends twice a week. How \\nmany pages does he write \\na year?\\nAnswer: 624\\nQuestion: James has 30 \\nteeth. His dentist drills 4 of \\nthem and caps 7 more \\nteeth than he drills. What \\npercentage of James\\' teeth \\ndoes the dentist fix?\\nChain-of-Thought (CoT)\\nContrastive Chain-of-Thought\\nStandard Prompting\\nModel Input\\nModel Input\\nModel Input\\nModel Output\\nModel Output\\nModel Output\\nFigure 3: Overview of contrastive chain-of-thought (right), with comparison to common prompting methods.\\nPrompting Method\\nGSM8K\\nBamboogle\\nAvg.\\nStandard\\n27.4\\n11.2\\n19.3\\nChain-of-Thought\\n69.2\\n40.8\\n55.0\\nw/ Invalid Reasoning\\n76.0\\n45.6\\n60.8\\nw/ Incoherent Objects\\n79.6\\n53.6\\n66.6\\nw/ Incoherent Language\\n78.8\\n52.8\\n65.8\\nw/ Irrelevant Objects\\n79.8\\n48.8\\n64.3\\nw/ Irrelevant Language\\n80.2\\n49.6\\n64.9\\nTable 1: Preliminary results on the effect of contrastive\\ndemonstrations for chain of thought.\\non GSM8K and Bamboogle. This suggests that\\nlanguage models are better able to learning step-\\nby-step reasoning when provided with both valid\\nand invalid rationales. Hence, we believe that con-\\ntrastive demonstrations have the potential to greatly\\nenhance language model reasoning ability.\\n3\\nContrastive Chain of Thought\\nChain-of-thought (CoT) prompting, as evidenced\\nby prior research, has indeed elevated the reasoning\\ncapabilities of large language models (Wei et al.,\\n2022b). However, a comprehensive understand-\\ning of this phenomenon is still lacking. Although\\nlogically sound reasoning appears to be inherently\\ncrucial for chain of thought, prior studies surpris-\\ningly reveal minimal impact when employing in-\\nvalid demonstrations. To this end, based on our\\npreliminary study in Section 2, we found that pro-\\nviding both valid and invalid reasoning demonstra-\\ntions in a “contrastive” manner greatly improves\\nreasoning performance. However, this approach\\nmay not generalize well to new tasks, as it requires\\nmanual construction of the invalid rationales.\\nThus, we propose a general prompting method\\nknown as contrastive chain of thought, which\\nincludes automatic construction of contrastive\\ndemonstrations. Figure 3 presents an overview of\\nour approach. Specifically, the language model is\\nprovided with the question, ground truth answer ex-\\nplanation and incorrect answer explanation. Com-\\npared to standard prompting, our method enables\\nmodels to perform more complex reasoning by de-\\ncomposing problems into intermediate steps. Com-\\npared to conventional chain-of-thought prompting,\\nour method contrasts the valid and invalid answer\\nexplanations, guiding the model to generate more\\naccurate reasoning chains.\\nConcretely, given a small set of n in-context\\ndemonstration examples D = {E1, . . . , E|n|}, and\\na query Q, the goal of the model is to generate a\\nsuitable answer A. For standard prompting, the\\ndemonstration examples consist of just the ques-\\ntion and answer, i.e., Ej = (Qj, Aj). On the other\\nhand, chain-of-thought is a more advanced prompt-\\ning method that guides the model with intermediate\\nPrompting Method\\nArithmetic Reasoning\\nFactual QA\\nGSM8K\\nAQuA\\nGSM-Hard\\nSVAMP\\nASDIV\\nBamboogle\\nStrategyQA\\nStandard\\n27.4\\n29.5\\n11.2\\n69.3\\n75.8\\n12.0\\n59.4\\nCoT\\n69.2\\n53.5\\n33.8\\n67.2\\n70.8\\n40.8\\n55.8\\nContrastive CoT\\n79.0 (+9.8)\\n57.5 (+3.9)\\n44.2 (+10.4)\\n81.6 (+14.4)\\n84.4 (+13.6)\\n56.8 (+16.0)\\n66.2 (+10.4)\\nStandard-SC\\n28.0\\n29.9\\n11.0\\n69.0\\n76.0\\n11.2\\n59.6\\nCoT-SC\\n71.0\\n55.9\\n34.0\\n71.6\\n74.0\\n40.8\\n57.0\\nContrastive CoT-SC\\n86.2 (+15.2)\\n71.7 (+15.7)\\n50.0 (+16.0)\\n85.2 (+13.6)\\n89.6 (+15.6)\\n58.4 (+17.6)\\n69.6 (+12.6)\\nTable 2: Main evaluation results for contrastive chain-of-thought on several reasoning tasks.\\nDataset\\nType\\n|Train|\\n|Test|\\nGSM8K\\nArithmetic Reasoning\\n4\\n500\\nAQuA\\nArithmetic Reasoning\\n4\\n254\\nGSM-Hard\\nArithmetic Reasoning\\n4\\n500\\nSVAMP\\nArithmetic Reasoning\\n4\\n500\\nASDIV\\nArithmetic Reasoning\\n4\\n500\\nBamboogle\\nFactual QA\\n4\\n125\\nStrategyQA\\nFactual QA\\n4\\n500\\nTable 3: Details of datasets used.\\nreasoning steps T. As shown in the figure above,\\nthe reasoning steps T typically consist of multi-\\nple sentences where each sentence describes one\\nreasoning step. Hence, chain-of-thought prompt-\\ning examples consist of the question, reasoning\\nsteps, and final answer, i.e., Ej = (Qj, Tj, Aj).\\nHowever, the model does not know what faults\\nto avoid in conventional chain-of-thought, which\\ncould lead to increased mistakes and error prop-\\nagation. Hence, our contrastive chain of thought\\nmethod provides both the correct and incorrect rea-\\nsoning steps in the demonstration examples, i.e.,\\nEj = (Qj, Tj,+, Aj,+, Tj,−, Aj,−).\\nTo obtain the correct reasoning steps T+ for the\\ndemonstration examples, we use the annotated ex-\\namples from the previous chain-of-thought works.\\nFor the incorrect reasoning steps T−, we automati-\\ncally construct it from the correct reasoning steps\\nT+, based on the \"Incoherent Objects\" category in\\nSection 2. Concretely, we use an existing entity\\nrecognition model5 to extract the object spans such\\nas numbers, equations, or persons from a given\\nchain-of-thought rationale. Consequently, we ran-\\ndomly shuffle the position of the objects within the\\nrationale, thus constructing a rationale with inco-\\nherent bridging objects. Note that when testing\\nwith a new question, only the question and demon-\\nstration examples are provided to the model, and\\nthe model must generate its own reasoning steps\\n5https://spacy.io/models/en#en_core_web_trf\\nbefore producing the final answer.\\n4\\nExperiments\\n4.1\\nExperimental Setup\\nWe focus our study on two main types of reasoning\\ntasks: arithmetic reasoning and factual question\\nanswering (QA). For arithmetic reasoning, we con-\\nduct experiments on a range of datasets including\\nGSM8K (Cobbe et al., 2021), AQuA (Ling et al.,\\n2017), GSM-Hard (Gao et al., 2023), SVAMP (Pa-\\ntel et al., 2021), and ASDIV (Miao et al., 2020).\\nFor factual QA, we include two datasets: Bam-\\nboogle (Press et al., 2023) and StrategyQA (Geva\\net al., 2021). To maintain a reasonable computing\\nbudget, we limit each dataset to a maximum of\\n500 test samples through random sampling. For\\ndatasets that contain less than 500 test samples, we\\ninstead use all available test samples. The datasets’\\ndetails are included in Table 3. Regarding model\\nand prompting details, we use the same experimen-\\ntal setup as for our preliminary study in Section\\n2.\\n4.2\\nMain Results\\nTo assess the effectiveness of our method, we eval-\\nuate on several reasoning tasks and report the main\\nresults in Table 2. Our main findings are as follows:\\nContrastive CoT demonstrates consistent im-\\nprovements\\nover\\nconventional\\nCoT.\\nCon-\\ntrastive CoT consistently outperforms conventional\\nCoT across the datasets in both arithmetic and fac-\\ntual reasoning categories. Notably, we observe\\nsubstantial gains of more than 10 points on GSM-\\nHard, SVAMP, ASDIV, Bamboogle and Strate-\\ngyQA. Thus, the consistent and significant perfor-\\nmance improvements demonstrate the general ef-\\nfectiveness of our proposed method. As contrastive\\nchain of thought can be automatically constructed\\nfrom existing rationales, the annotation cost is the\\nsame as conventional chain of thought. Hence, it\\ncan be viewed as a general enhancement of chain\\nof thought.\\nContrastive CoT is more effective when ap-\\nplied with self-consistency.\\nAs self-consistency\\n(Wang et al., 2022) is a popular decoding strategy\\nto boost the chain-of-thought performance of large\\nlanguage models, we are interested to see if con-\\ntrastive chain of thought can benefit similarly from\\nself-consistency. In general, we observe that self-\\nconsistency further enhances the performance of\\ncontrastive CoT. This enhancement is particularly\\nevident in the case of the AQuA dataset. While con-\\ntrastive CoT alone results in a modest performance\\nimprovement of 4.0%, applying self-consistency\\namplifies this gain significantly, achieving an addi-\\ntional improvement of 14.2%.\\n5\\nRelated Work\\nLarge Language Models\\nRecent developments\\nin large language models have shown that mas-\\nsively scaling the size and training data of models\\ncan greatly improve generalization (Kaplan et al.,\\n2020). Notably, large language models have been\\nshown to generalize to new tasks when given suit-\\nable prompts and demonstrations (Brown et al.,\\n2020). This has brought about a new paradigm of\\nleveraging language models for tasks without the\\nneed for additional training (Liu et al., 2023). How-\\never, simply scaling language models has not been\\nsufficient to attain good performance on challeng-\\ning tasks such as arithmetic reasoning and factual\\nquestion answering (Wei et al., 2022b). Hence, in\\nthis work, we focus on enhancing the reasoning\\nability of large language models through prompts.\\nChain of Thought\\nChain-of-thought prompting\\nwas introduced by Wei et al. (2022b) to enhance\\nlanguage model reasoning by generating interme-\\ndiate steps. Notably, this has inspired numerous\\nworks that build upon this direction of step-by-\\nstep reasoning. For instance, automatic chain-of-\\nthought (Zhang et al., 2023) was proposed to ad-\\ndress the challenges in manually annotating chain-\\nof-thought demonstrations. On the other hand, it\\nwas shown that specific prompts such as “Let’s\\nthink step-by-step” can enable language models\\nto perform chain-of-thought in a zero-shot man-\\nner, without any demonstrations (Kojima et al.,\\n2022). In addition, challenging problems can be de-\\ncomposed into multiple sub-problems (Zhou et al.,\\n2023), or even into code programs that can be au-\\ntomatically executed (Gao et al., 2023). Despite\\nthe progress in chain-of-thought on multiple fronts,\\nwe still lack a rigorous understanding of the under-\\nlying mechanism (Turpin et al., 2023; Feng et al.,\\n2023). In this work, inspired by the findings of pre-\\nvious works regarding invalid demonstrations, we\\npropose contrastive chain-of-thought to enhance\\nlanguage model reasoning. As contrastive chain-\\nof-thought leverages both valid and invalid reason-\\ning demonstrations, we believe this may encour-\\nage other researchers to fundamentally rethink the\\nchain-of-thought process.\\nLearning from Negative Examples\\nWhile\\nchain-of-thought prompting typically involves only\\nvalid demonstrations, it is not clear whether in-\\nvalid demonstrations can also benefit the reason-\\ning process (Wang et al., 2023).\\nOn the other\\nhand, learning from negative or invalid samples\\nis not new. For instance, contrastive learning is\\na well-established deep learning approach that en-\\ncourages models to distinguish between “positive”\\nand “negative” samples, thus learning better rep-\\nresentations (Khosla et al., 2020). Similarly, rein-\\nforcement learning from human feedback (RLHF)\\ntrains a reward model based on positive and neg-\\native samples of human preference data (Ouyang\\net al., 2022; Christiano et al., 2017). Hence, in-\\nspired by the previous approaches, we propose con-\\ntrastive chain-of-thought, a general enhancement\\nof chain-of-thought prompting, by enabling mod-\\nels to learn from both valid and invalid reasoning\\ndemonstrations.\\n6\\nConclusions\\nIn this work, we have explored the effect of leverag-\\ning invalid reasoning demonstrations for enhancing\\nchain of thought. Through our preliminary study\\non different invalid chain-of-thought categories, we\\nfound that providing both valid and invalid demon-\\nstrations in a contrastive manner greatly improves\\nreasoning ability in language models. To overcome\\nthe challenge of manually annotating invalid ratio-\\nnales, we propose contrastive chain of thought, a\\ngeneral prompting method which can automatically\\nconstruct contrastive demonstrations from existing\\nrationales. Through experiments on several reason-\\ning tasks, we find contrastive chain of thought to be\\na general enhancement of chain-of-thought prompt-\\ning. Further investigation into alternative forms of\\nchain-of-thought prompting will hopefully inspire\\nfuture advancements in language-based reasoning.\\nReferences\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nClark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners.\\nIn Ad-\\nvances in Neural Information Processing Systems,\\nvolume 33, pages 1877–1901. Curran Associates,\\nInc.\\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\\ntic, Shane Legg, and Dario Amodei. 2017. Deep\\nreinforcement learning from human preferences. In\\nAdvances in Neural Information Processing Systems,\\nvolume 30. Curran Associates, Inc.\\nZheng Chu, Jingchang Chen, Qianglong Chen, Weijiang\\nYu, Tao He, Haotian Wang, Weihua Peng, Ming Liu,\\nBing Qin, and Ting Liu. 2023. A survey of chain of\\nthought reasoning: Advances, frontiers and future.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\\nNakano, Christopher Hesse, and John Schulman.\\n2021. Training verifiers to solve math word prob-\\nlems. CoRR, abs/2110.14168.\\nNathan Cooper, Carlos Bernal-Cárdenas, Oscar Cha-\\nparro, Kevin Moran, and Denys Poshyvanyk. 2021.\\nIt takes two to tango: Combining visual and textual\\ninformation for detecting duplicate video-based bug\\nreports. CoRR, abs/2101.09194.\\nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye,\\nDi He, and Liwei Wang. 2023. Towards revealing\\nthe mystery behind chain of thought: A theoretical\\nperspective. In Thirty-seventh Conference on Neural\\nInformation Processing Systems.\\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\\nham Neubig. 2023. PAL: Program-aided language\\nmodels. In Proceedings of the 40th International\\nConference on Machine Learning, volume 202 of\\nProceedings of Machine Learning Research, pages\\n10764–10799. PMLR.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics, 9:346–\\n361.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\\nScaling laws for neural language models. CoRR,\\nabs/2001.08361.\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\\nSarna,\\nYonglong\\nTian,\\nPhillip\\nIsola,\\nAaron\\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\\npervised contrastive learning. In Advances in Neural\\nInformation Processing Systems, volume 33, pages\\n18661–18673. Curran Associates, Inc.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\\nguage models are zero-shot reasoners. In Advances\\nin Neural Information Processing Systems.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\\nsom. 2017. Program induction by rationale genera-\\ntion: Learning to solve and explain algebraic word\\nproblems. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 158–167, Vancouver,\\nCanada. Association for Computational Linguistics.\\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang,\\nMingu Lee, Roland Memisevic, and Hao Su. 2023.\\nDeductive verification of chain-of-thought reasoning.\\nIn Thirty-seventh Conference on Neural Information\\nProcessing Systems.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\\ntrain, prompt, and predict: A systematic survey of\\nprompting methods in natural language processing.\\nACM Comput. Surv., 55(9).\\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\\n2020. A diverse corpus for evaluating and developing\\nEnglish math word problem solvers. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 975–984, Online.\\nAssociation for Computational Linguistics.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Gray, John\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\nMaddie Simens, Amanda Askell, Peter Welinder,\\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\\nTraining language models to follow instructions with\\nhuman feedback. In Advances in Neural Information\\nProcessing Systems.\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\\n2021. Are NLP models really able to solve simple\\nmath word problems? In Proceedings of the 2021\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, pages 2080–2094, Online.\\nAssociation for Computational Linguistics.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A. Smith, and Mike Lewis. 2023. Measuring\\nand narrowing the compositionality gap in language\\nmodels.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\\nMillican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susan-\\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\\ncob Menick, Albin Cassirer, Richard Powell, George\\nvan den Driessche, Lisa Anne Hendricks, Mari-\\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\\nAidan Clark, Diego de Las Casas, Aurelia Guy,\\nChris Jones, James Bradbury, Matthew Johnson,\\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\\nlanguage models: Methods, analysis & insights from\\ntraining gopher.\\nMiles Turpin, Julian Michael, Ethan Perez, and\\nSamuel R. Bowman. 2023. Language models don’t\\nalways say what they think: Unfaithful explanations\\nin chain-of-thought prompting. In Thirty-seventh\\nConference on Neural Information Processing Sys-\\ntems.\\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2023.\\nTowards understanding chain-of-thought prompting:\\nAn empirical study of what matters. In Proceedings\\nof the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\npages 2717–2739, Toronto, Canada. Association for\\nComputational Linguistics.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Huai hsin Chi, and Denny Zhou. 2022. Self-\\nconsistency improves chain of thought reasoning in\\nlanguage models. ArXiv, abs/2203.11171.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\\n2022a. Emergent abilities of large language models.\\nTrans. Mach. Learn. Res., 2022.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. 2022b. Chain of thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\\nSmola. 2023. Automatic chain of thought prompting\\nin large language models. In The Eleventh Interna-\\ntional Conference on Learning Representations.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nClaire Cui, Olivier Bousquet, Quoc V Le, and Ed H.\\nChi. 2023. Least-to-most prompting enables com-\\nplex reasoning in large language models. In The\\nEleventh International Conference on Learning Rep-\\nresentations.\\n'),\n",
       " SearchResult(title='Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models', published='2024-03-23', summary=\"With the widespread use of language models (LMs) in NLP tasks, researchers\\nhave discovered the potential of Chain-of-thought (CoT) to assist LMs in\\naccomplishing complex reasoning tasks by generating intermediate steps.\\nHowever, human thought processes are often non-linear, rather than simply\\nsequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)\\nreasoning, which models human thought processes not only as a chain but also as\\na graph. By representing thought units as nodes and connections between them as\\nedges, our approach captures the non-sequential nature of human thinking and\\nallows for a more realistic modeling of thought processes. GoT adopts a\\ntwo-stage framework with an additional GoT encoder for thought graph\\nrepresentation and fuses the graph representation with the original input\\nrepresentation through a gated fusion mechanism. We evaluate GoT's performance\\non a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task\\n(ScienceQA). Our model achieves significant improvement over the strong CoT\\nbaseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59%\\nusing the T5-base model over the state-of-the-art Multimodal-CoT on the\\nScienceQA test set.\", authors=['Yao Yao', ' Zuchao Li', ' Hai Zhao'], page_content='Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in\\nLanguage Models\\nYao Yao1,2, Zuchao Li3,∗and Hai Zhao1,2,∗\\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\\n2MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\\n3National Engineering Research Center for Multimedia Software,\\nSchool of Computer Science, Wuhan University, Wuhan, 430072, P. R. China\\nyaoyao27@sjtu.edu.cn, zcli-charlie@whu.edu.cn,\\nzhaohai@cs.sjtu.edu.cn\\nAbstract\\nWith the widespread use of language mod-\\nels (LMs) in NLP tasks, researchers have\\ndiscovered the potential of Chain-of-thought\\n(CoT) to assist LMs in accomplishing com-\\nplex reasoning tasks by generating intermedi-\\nate steps. However, human thought processes\\nare often non-linear, rather than simply se-\\nquential chains of thoughts.\\nTherefore, we\\npropose Graph-of-Thought (GoT) reasoning,\\nwhich models human thought processes not\\nonly as a chain but also as a graph. By repre-\\nsenting thought units as nodes and connections\\nbetween them as edges, our approach captures\\nthe non-sequential nature of human thinking\\nand allows for a more realistic modeling of\\nthought processes. GoT adopts a two-stage\\nframework with an additional GoT encoder\\nfor thought graph representation and fuses the\\ngraph representation with the original input\\nrepresentation through a gated fusion mech-\\nanism. We evaluate GoT’s performance on a\\ntext-only reasoning task (AQUA-RAT) and a\\nmultimodal reasoning task (ScienceQA). Our\\nmodel achieves significant improvement over\\nthe strong CoT baseline on the AQUA-RAT test\\nset and boosts accuracy from 85.19% to 87.59%\\nusing the T5-base model over the state-of-the-\\nart Multimodal-CoT (Zhang et al., 2023) on the\\nScienceQA test set. Our code is publicly avail-\\nable at https://github.com/Zoeyyao27/Graph-\\nof-Thought\\n1\\nIntroduction\\nIn the field of human cognition, it has long been rec-\\nognized that the human thought process is far more\\ncomplex and non-linear than could be captured by\\na simple, sequential chain of thoughts (Barsalou,\\n∗Corresponding author. This research was supported\\nby the National Natural Science Foundation of China (No.\\n62306216), the Natural Science Foundation of Hubei Province\\nof China (No. 2023AFB816), the Fundamental Research\\nFunds for the Central Universities (No. 2042023kf0133), the\\nJoint Research Project of Yangtze River Delta Science and\\nTechnology Innovation Community (No. 2022CSJGG1400).\\n1999). Human thinking is often characterized by\\nits ability to make sudden leaps and connections\\nbetween seemingly unrelated ideas, which can lead\\nto novel insights and solutions. This non-linear,\\njumping thought process is a hallmark of human\\ncreativity, reasoning, and problem-solving abilities.\\nHowever, it also poses a significant challenge for\\ncognitive modeling and understanding.\\nRecently, Large Language Models (LLMs) have\\nbeen advancing at an unprecedented pace. With\\nthe emergence of breakthroughs such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022), and GPT-4 (OpenAI, 2023), the field of\\nnatural language processing has entered a new\\nera of possibilities. Recent studies (Wei et al.,\\n2022a; Wang et al., 2022; Zhang et al., 2022) have\\nshown that the reasoning ability of LLMs can be\\nunlocked by Chain-of-Thought (CoT) prompting.\\nCoT prompting involves a series of intermediate\\nnatural language rationales that lead to the final\\nanswer. In addition, Zhang et al. (2023) have in-\\ntroduced Multimodal-CoT, which combines both\\nlanguage and visual modalities to help surpass the\\nlimitations of textual information. More detailed\\nrelated works can be found in Appendix A.\\nPrevious works on Chain-of-Thought (CoT)\\nprompting, which have been limited to textual and\\nvisual information, often represented the human\\nreasoning process as sequential thought chains.\\nThis approach overlooks the modeling of humans’\\njumping thought process and neglects to incorpo-\\nrate the complex structural information of reason-\\ning thoughts into the model.\\nConcurrent work\\nTree-of-thoughts (ToT) (Yao et al., 2023) divides\\nthoughts into thought units and models them as a\\ntree-like search process.\\nNevertheless, human cognition transcends this\\ntree structure, exhibiting intricate graph-like for-\\nmations. Our perspective diverges further as we\\nbelieve that the human intellect is capable of craft-\\ning elaborate thought graphs founded upon linear\\narXiv:2305.16582v2  [cs.CL]  23 Mar 2024\\nDo ferns produce seeds?\\nText Features\\n(A) Yes\\n(B) No\\nThis diagram shows the life cycle of \\na fern.\\nVision Features (Optional)\\nGraph-of-Thought Features\\nproduce\\nseeds\\nferns\\nshows\\nlife \\ncycle\\nof\\ndiagram\\nFern plants reproduce using both asexual reproduction \\nand sexual reproduction … The heart-shaped plant \\nbegins the fern\\'s sexual reproduction stage … The mature \\nfern can make spores and begin the fern life cycle again.\\nRationale\\nFerns do not produce seeds. Mature ferns produce spores, \\nand heart-shaped plants produce eggs and sperm.\\nAnswer\\nThe answer \\nis (B)\\nGraph-of-Thought with Rationale\\nproduce\\nseeds\\nferns\\nshows\\nlife \\ncycle\\nof\\ndiagram\\nhas\\nsexual \\nproduction\\nstage\\nFigure 1: An example of GoT reasoning. Vision features are optional and are only required in multimodal reasoning.\\nthoughts. Therefore, we aim to enable the concur-\\nrent assimilation of linear and nonlinear cognitive\\nprocesses, surpassing the mere generation of seg-\\nmented thought units. To address the above limi-\\ntation, different from ToT, we propose the Graph-\\nof-Thought (GoT), a novel approach to modeling\\nhuman thought processes not only as a chain but\\nalso as a graph. Our method is based on the assump-\\ntion that the human mind works by connecting and\\nrecombining ideas in a non-sequential, graph fash-\\nion, rather than following a strict sequential chain.\\nBy representing thought units as nodes and connec-\\ntions between thoughts as edges, GoT captures the\\nrich, non-sequential nature of human thinking and\\nallows for a more realistic and logical modeling of\\nreasoning processes.\\nAn example of GoT reasoning is shown in Fig-\\nure 1. Inspired by Multimodal-CoT (Zhang et al.,\\n2023), we have adopted a two-stage reasoning\\nframework. It first generates rationales and then\\ngenerates the final answer based on the predicted ra-\\ntionales. In addition to text features, graph features\\nof GoT are integrated during the rationale genera-\\ntion and answer inference. Specifically, GoT is first\\nconstructed with an Extract-Cluster-Coreference\\n(ECC) process, which simulates the deductive pro-\\ncess in human reasoning. We have used T5 (Raffel\\net al., 2020a) pre-trained language model as our\\nbackbone model. GoT is encoded with a graph\\nattention network and then fused with the original\\nrepresentation via a gated fusion network.\\nFurthermore, we have also presented a multi-\\nmodal GoT, which integrates not only text features\\nand GoT features but also visual features. For our\\nexperiments, we have used both FLAN-Alpaca\\n1 (T5)-base and FLAN-Alpaca (T5)-large as our\\nbackbone models.\\nWe implement GoT as a two-stage framework\\nand fine-tuning language models and integrating\\ntext, thought graph, and vision features for a more\\nrealistic and accurate reasoning process.\\nGoT\\ndemonstrates exceptional performance on both text-\\nonly AQUA-RAT (Ling et al., 2017) and multi-\\nmodal ScienceQA (Lu et al., 2022) benchmarks,\\nsurpassing the accuracy of online system Chat-\\nGPT (OpenAI, 2023) by 9.28%, strong baseline\\nMultimodal-CoT (Zhang et al., 2023) by 2.40%,\\nand even exceeding human performance, establish-\\ning a new state-of-the-art on ScienceQA test set\\nwith far fewer parameters.\\n2\\nGraph-of-Thought\\nThe overview of our proposed GoT can be seen\\nin Figure 2. Inspired by Multimodal-CoT (Zhang\\net al., 2023), GoT also adopts a two-stage frame-\\nwork. (1) Rationale generation stage: In the first\\nstage, the model generates rationales based on the\\ninput text (including question, context, and choices)\\nthe vision features, and the generated thought graph\\ncorresponding to the input text. For multi-modal\\ntasks (Zhang et al., 2023; Zhang and Zhang, 2023;\\nHuang et al., 2023; Peng et al., 2023), it is a com-\\nmon practice to use different encoders to process\\ninputs from different modalities and a straightfor-\\n1https://github.com/declare-lab/flan-alpaca. FLAN-Alpaca\\nis developed by fine-tuning T5 model on the Flan collection\\nThought Graph\\nImage (Optional)\\nGraph-of-Thought \\nConstructor\\nInput Text \\nQuestion: Do ferns \\nproduce seeds?\\nChoices: (A) Yes (B) No\\nContext: This diagram \\nshows the life cycle of \\na fern.\\nPredicted \\nRationales\\nInput \\nEncoder\\nGoT\\nEncoder\\nText\\nencoder\\nVision \\nencoder\\nGraph \\nAttention \\nNetwork \\nTransformer\\nEncoder\\nFeature\\nExtractor\\nCross\\nAttention\\nCross\\nAttention\\nGated\\nFusion\\nLayer\\nTransformer\\nDecoder\\nStage 1\\nPredict Rationales\\nLecture：Fern plants reproduce \\nusing both asexual reproduction \\nand sexual reproduction…\\nSolution:  Ferns do not produce \\nseeds. Mature ferns produce \\nspores…\\nThe answer is (B).\\nDecoder\\nOutput \\nFeature Fusion\\nStage 2\\nStage 2\\nPredict Answers\\nFigure 2: Graph-of-Thought framework overview\\nward and versatile approach is to employ encoder-\\ndecoder models. Therefore, GoT employs inde-\\npendent encoders to encode input data for each\\nmodality. We use a Transformer encoder to encode\\ninput text, a vision encoder to encode an image, and\\na graph attention network to encode the thought\\ngraph. The encoded features are further passed\\ninto cross-attention to align text tokens with image\\npatches and graph nodes, respectively. We then\\nuse a gated fusion layer to fuse these three features\\nfurther and pass them into the Transformer decoder\\nto predict the target rationales. (2) Answer gener-\\nation stage: The second stage aims at generating\\nthe final answer and is largely similar to the first\\nstage. The main difference is that the input text\\nis concatenated with the predicted rationales from\\nthe first stage. It is worth noting that the above\\nprocess describes a general multimodal reasoning\\nframework. However, for text-only reasoning tasks,\\nthere are no image features, so the image encod-\\ning and vision feature fusion processes mentioned\\nabove can be omitted. In the following section,\\nwe will provide a detailed exposition of the two\\nkey steps of our GoT reasoning framework: GoT\\nconstruction and GoT encoding and feature fusion.\\n2.1\\nGoT Construction\\nGoT employs thought graphs to simulate human de-\\nductive reasoning, thereby modeling humans’ abil-\\nity for leaps of thought. Our aim is to reflect the\\nmost fundamental deduction process by construct-\\ning a thought graph. If we have evidence that x →\\ny and y →z, then it follows that x →z. In Fig-\\nure 3, the deduction reasoning can be formulated\\nas follows: Earthquake\\ncomes from\\n−→\\n{earth, quake},\\n{earth, quake} means\\n−→{ground, shake}. It is easy to\\nreason that Earthquake−→{ground, shake}.\\nWe\\npropose\\na\\nnovel\\nExtract-Clustering-\\nCoreference (ECC) process to construct thought\\ngraphs.\\nECC first extracts deductive triplets\\nT = {ti = (ti\\nx, ti\\ny, ti\\nz)} as the discrete raw graph,\\nwhere ti\\nx, ti\\ny, and ti\\nz are thought units of the i-th\\ntriplet, and there exists an edge ei\\nxy between ti\\nx and\\nti\\ny, and an edge ei\\nyz between ti\\ny and ti\\nz. Then, ECC\\nclusters the nodes that refer to the same mentions\\nto conduct coreference resolution. Specifically,\\nwe replace every graph node that belongs to a\\ncoreference cluster with the most representative\\nmention in the cluster. By adopting this technique,\\nour model is better equipped with denser thought\\ngraphs and the ability for deductive reasoning. The\\ndetailed algorithm is illustrated in Algorithm 1.\\nIn GoT construction, during the rationale gen-\\neration stage, the input text consists of concate-\\nnated question, context, and choices. In multimodal\\nGoT, image caption (Lu et al., 2022) is appended\\nto the input text for GoT to incorporate image in-\\nThe word earthquake \\ncomes from the words \\nearth and quake. The word \\nearth means ground, and \\nthe word quake means to \\nshake.\\nEarthquake\\ncomes \\nfrom\\nearth\\nquake\\nmeans\\nground\\nshake\\nGoT\\nRationales\\nFigure 3: Graph-of-Thought deduction example\\nformation. During the answer inference stage, the\\npredicted rationales from the rationale generation\\nstage are further concatenated with the input text\\nfor corresponding GoT construction.\\nIn our implementation of ECC process, inspired\\nby (Chen and Yang, 2021), we utilize open in-\\nformation extraction (OpenIE) systems 2 (Angeli\\net al., 2015) to extract subject-verb-object triplets\\nas thought unit nodes. We apply coreference reso-\\nlution to the extracted nodes using the Stanford\\nCoreNLP system (Manning et al., 2014).\\nThe\\nconstructed thought graph is denoted as G(N, E),\\nwhere N represents the nodes extracted by OpenIE\\nand E represents the adjacency matrix. Rows and\\ncolumns correspond to the nodes in the graph, and\\nif there is an edge between two nodes, the corre-\\nsponding matrix element is 1; otherwise, it is 0.\\nAlgorithm 1 ECC process\\nInput: Input text S\\nOutput: Thought graph G(N, E)\\nExtract deductive triplet set T from S\\nT = {t0, t1, ..., tn}, ti = (ti\\nx, ti\\ny, ti\\nz)\\nfor every triplet ti ∈T do\\nNr ←Nr ∪{ti\\nx, ti\\ny, ti\\nz}\\nEr ←Er ∪{ei\\nxy, ei\\nyz}\\nend for\\nextract coreference clusters C for Nr\\nfor every node ni ∈Nr do\\nif ni ∈∀cj ∈C then\\nn∗\\nj ←most representative mention in cj\\nN ←N ∪{n∗\\nj}\\nend if\\nend for\\nReconnect N based on Er to construct E\\nreturn N , E\\n2https://github.com/philipperemy/Stanford-OpenIE-\\nPython\\n2.2\\nGoT Encoding and Integration\\nGoT reasoning utilizes separate encoders to encode\\ninput data for each modality. The thought graph\\nis encoded using a graph attention network, while\\nthe input text is encoded using a Transformer en-\\ncoder. In multimodal GoT reasoning, the image is\\nencoded using an additional vision encoder.\\n2.2.1\\nBase Encoder\\nText Encoder\\nFor text representation, we use the\\nTransformer encoder (e.g. T5 (Raffel et al., 2020a))\\nto encode the input text. Given input sentence S =\\n{w0, ..., wl}, we extract the hidden states from the\\nlast layer of the Transformer encoder to obtain the\\ntext representation HT :\\nHT = {h0, h1, ..., hl} = Encodertext(S)\\n(1)\\nwhere hi is the hidden representation of token i and\\nl represents the length of the text input.\\nVision Encoder (Optional)\\nFor multimodal rea-\\nsoning with vision modality, following (Zhang\\net al., 2023), we extract patch-level features of\\nimage I using readily available vision extraction\\nmodel as vision encoder Encodervision and then\\nemploy a trainable projection matrix WI to project\\nthe extracted features into the vision representation\\nHI which have the same shape with HT .\\nHI = WIEncodervision(I)\\n(2)\\n2.2.2\\nGoT Encoder\\nNode Embedding\\nWe first use special tokens\\n<s> and </s> to highlight every thought graph\\nnode.\\nSpecifically, for node set with j nodes\\nN = {n0, ...nj} , we construct the node input as p\\nand then feed the p into the same text encoder and\\nutilize the output representation of the special token\\n<s> as the initial node representation. Formally,\\np = [<s>, n0, </s>, ..., <s>, nj, </s>]\\n(3)\\n[hs\\n0, hn\\n0, he\\n0, ..., hs\\nj, hn\\nj , he\\nj] = Encodertext(p)\\n(4)\\nwhere the hs\\ni and he\\ni ∈RD are the representation\\nof <s> and </s> for node ni respectively, D is\\nthe dimension of node embedding, and the hn\\ni =\\n{hn\\ni,1, ..., hn\\ni,m} is the representations of node ni\\nwith m tokens. we use the hs\\ni to represent the node\\nrepresentation of ni.\\nDropout\\nGoT input\\nG 𝑁, 𝐸\\nGraph \\nAttention Layer\\nGraph \\nAttention Layer\\nConcatenate\\nDropout\\nGraph \\nAttention Layer\\nFFNN\\nLayernorm\\nGoT representation\\nMulti-head \\nattention\\nResidual connection\\nℎ𝑔′\\nℎ𝑔′\\n𝐻𝐺\\n…\\nFigure 4: Architecture of GoT encoder\\nGAT Encoder\\nWe employ a graph attention net-\\nwork (GAT) (Velickovic et al., 2018; Chen and\\nYang, 2021) to encode the thought graph. For every\\nnode ni in graph G(N, E), the graph attention\\nlayer is designed as:\\naij = Attention(\\n\\x02\\nWhs\\ni||Whs\\nj\\n\\x03\\n)\\n(5)\\nqij = LeakyReLU (aij)\\n(6)\\nαij = Softmax(qij) =\\nexp (qij)\\nP\\nk∈Ki exp (qik)\\n(7)\\nhg′\\ni = GELU\\n\\uf8eb\\n\\uf8edX\\nj∈Ki\\nαijWhs\\nj\\n\\uf8f6\\n\\uf8f8\\n(8)\\nwhere || denotes concatenate operation, the W is\\na trainable weight and the set Ki contains the node\\nni’s neighbours in thought graph G. Our graph\\nattention layer first employed a shared attention\\nmechanism Attention(.) : RD′ × RD′ →R to\\ncompute the attention weights, where D′ is the\\nattention layer output dimension. The attention\\nweights aij measures the importance of node ni’s\\nfeatures to nj’s features. By only calculating the\\nattention weights between nodes who are neigh-\\nbours, our graph attention layer demonstrates the\\nability to perceive structural information of graphs.\\nIn our implementation, we adopt a single-layer\\nfeed-forward neural network (FFNN) as the atten-\\ntion mechanism which is both simple and straight-\\nforward.\\nFigure 4 shows the architecture of our GoT en-\\ncoder. Our GoT encoder employs a multi-head\\ngraph attention layer, following (Velickovic et al.,\\n2018), we concatenate the output of each graph\\nattention layer and further pass it to a output graph\\nattention layer with the same architecture:\\nhg′\\ni = ∥K\\nk=1GELU\\n\\uf8eb\\n\\uf8edX\\nj∈Ni\\nαk\\nijWkhs\\nj\\n\\uf8f6\\n\\uf8f8\\n(9)\\nhg′′\\ni\\n= GELU\\n\\uf8eb\\n\\uf8edX\\nj∈Ni\\nαijWhg′\\nj\\n\\uf8f6\\n\\uf8f8\\n(10)\\nwhere K is the number of attention heads, || is\\nthe concatenate operation, and n is the number of\\nnodes in thought graph. We then use a single-layer\\nfeed-forward neural network (FFNN) to obtain the\\nfinal thought graph embedding HG:\\nhg′′ = [hg′′\\n0 , ..., hg′′\\nn ];\\nHG = FFNN(hg′′)\\n(11)\\n2.3\\nFeature Fusion\\nAfter obtaining the encoded features, we use a\\nsingle head attention to align the text representa-\\ntion HT with image representation HI and thought\\ngraph representation HG, respectively. The image\\nattention output HI and thought graph attention\\noutput HG are calculated by:\\nHI = Softmax\\n\\x12HT HI⊤\\n√\\nd\\n\\x13\\nHI\\n(12)\\nHG = Softmax\\n\\x12HT HG⊤\\n√\\nd\\n\\x13\\nHG\\n(13)\\nwhere Q is HT and d is the dimension of HT .\\nWe take both KI and VI as HI and KG and VG as\\nHG. Please note that image representation is op-\\ntional and is only required for multimodal dataset.\\nNext, a gated fusion mechanism (Wu et al., 2021;\\nZhang et al., 2023; Li et al., 2022; Zhang et al.,\\n2020) is applied to combine the attention outputs\\nHI and HG with the text representation HT . The\\nfeature fusion output H can be calculated by:\\nλ =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nSigmoid\\n\\x00WT HT + WGHG\\x01\\ntext-only\\nSigmoid\\n\\x00WT HT + WIHI + WGHG\\x01\\nmultimodal\\nH =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n(1 −λ) · HT + λ · HG\\ntext-only\\n(1 −λ) · HT + λ · HI + λ · HG\\nmultimodal\\nwhere WT ,WI and WG are all trainable weights.\\nWe then input the fused feature output H into the\\ndecoder to predict the rationales or the final answer.\\n3\\nExperiments\\nDataset\\nWe evaluate our model on the text-only\\nAQUA-RAT (Ling et al., 2017) and multimodal\\nScienceQA benchmark (Lu et al., 2022). The de-\\ntailed dataset information and statistics are shown\\nin Appendix B.\\nModel Setup\\nIn our experiments, we used\\nT5 (Raffel et al., 2020a) as our basic model ar-\\nchitecture, including both T5-base and T5-large\\nmodel sizes. Specifically, to ensure a fair compar-\\nison, we initialized our model with the finetuned\\nT5 checkpoint FLAN-Alpaca 3 and used ViT-large\\nencoder (Dosovitskiy et al., 2021) for the vision\\nencoder, following (Zhang et al., 2023). We fine-\\ntuned the models for 100 epochs with a learning\\nrate of 5e-5. The detailed training parameters are\\navailable in Appendix C. We trained our models on\\nfour NVIDIA A800 80G GPUs.\\n4\\nResults and Discussion\\n4.1\\nMain Results\\nBaselines\\nFor AQUA-RAT, our baselines include:\\n(1) Zero-Shot and Zero-Shot-CoT LLMs (Kojima\\net al., 2022); (2) Few-Shot and Manual-CoT LLMs\\n(Wei et al., 2022b) and Auto-CoT (Zhang et al.,\\n2022) (The above baselines all use the text-davinci-\\n002 version of GPT-3 with 175B parameters); (3)\\n3https://huggingface.co/declare-lab/flan-alpaca-base\\nFintuned LLMs: Calcformer-T5-L (Kadlˇcík et al.,\\n2023) which finetunes calculator-using T5-Large\\nmodel on the Calc-X collection. To have a fair com-\\nparison we also fine-tuned FLAN-Alpacabase and\\nFLAN-Alpacalarge on AQUA-RAT with traditional\\ntwo-stage CoT.\\nFor ScienceQA, following (Zhang et al., 2023;\\nLu et al., 2022), our adopted baselines include: (1)\\nVision question answering (VQA) baseline mod-\\nels (Yu et al., 2019; Anderson et al., 2018; Kim\\net al., 2018; Gao et al., 2019; Kim et al., 2021;\\nLu et al., 2021; Li et al., 2019, 2020); (2) Text-\\nto-text LLMs (Raffel et al., 2020b; Chen et al.,\\n2020) and (3) Text-to-text LLMs with CoT prompt-\\ning (Lu et al., 2022; Zhang et al., 2023). Both\\nUnifiedQA (Lu et al., 2022) and GPT-3.5 (Lu\\net al., 2022) use generated image captions to in-\\ncorporate vision semantics. Whereas, Mutimodal-\\nCoT (Zhang et al., 2023) injects generated image\\nfeatures into traditional CoT reasoning.\\nMODELS\\nTRAINING\\nSIZE\\nACC(%)\\nZero-Shot (Kojima et al., 2022)\\nzero-shot\\n175B\\n22.40\\nZero-Shot-CoT (Kojima et al., 2022)\\nzero-shot\\n175B\\n33.50\\nFew-Shot (Wei et al., 2022b)\\nfew-shot\\n175B\\n24.80\\nManual-CoT (Wei et al., 2022b)\\nfew-shot\\n175B\\n35.80\\nAuto-CoT (Zhang et al., 2022)\\nfew-shot\\n175B\\n36.50\\nCalcformer-T5-L (Kadlˇcík et al., 2023)\\ntrain-set\\n770M\\n27.20\\nFLAN-Alpacabase\\ntrain-set\\n223M\\n30.09 ± 1.12\\nGoT-T5base\\ntrain-set\\n223M\\n32.09 ± 1.62\\nFLAN-Alpacalarge\\ntrain-set\\n738M\\n33.73 ± 1.14\\nGoT-T5large\\ntrain-set\\n738M\\n34.48 ± 1.11\\nTable 1: Main test accuracy results (ACC%) of AQUA-\\nRAT. Size=backbone model size.\\nResults\\nThe rationales generation results can be\\nseen in Table 8 in Appendix D. The overall results\\nare reported in Table 1 and Table 2.\\nIn the AQUA-RAT dataset, our GoTbase model\\nattains a 0.78 enhancement in ROUGE-L scores for\\nrationale generation during the initial stage, outper-\\nforming the FLAN-Alpacabase model, which does\\nnot integrate GoT. For the answer generation phase,\\nthe GoTbase exhibits a substantial accuracy increase\\nof 2.00%, while the GoTlarge model records a 0.75%\\nenhancement. Compared to the 175B parameter\\nzero-shot and few-shot LLMs, our GoT-large, em-\\nploying just a 738M backbone model, achieves\\nresults remarkably close to those of Manual-CoT\\n(Wei et al., 2022b).\\nFor ScienceQA dataset, in rationale generation\\nstage, we can see from Table 8 that our model\\nachieves a ROUGE-L of 94.39 and outperforms\\nthe Mutimodal-CoTbase by 1.15. For the final an-\\nMODEL\\nTRAINING\\nSIZE\\nNAT\\nSOC\\nLAN\\nTXT\\nIMG\\nNO\\nG1-6\\nG7-12\\nAVG\\nHuman\\n-\\n-\\n90.23\\n84.97\\n87.48\\n89.60\\n87.50\\n88.10\\n91.59\\n82.42\\n88.40\\nVision question answering baselines\\nMCAN (Yu et al., 2019)\\ntrain-set\\n95M\\n56.08\\n46.23\\n58.09\\n59.43\\n51.17\\n55.40\\n51.65\\n59.72\\n54.54\\nTop-Down (Anderson et al., 2018)\\ntrain-set\\n70M\\n59.50\\n54.33\\n61.82\\n62.90\\n54.88\\n59.79\\n57.27\\n62.16\\n59.02\\nBAN (Kim et al., 2018)\\ntrain-set\\n112M\\n60.88\\n46.57\\n66.64\\n62.61\\n52.60\\n65.51\\n56.83\\n63.94\\n59.37\\nDFAF (Gao et al., 2019)\\ntrain-set\\n74M\\n64.03\\n48.82\\n63.55\\n65.88\\n54.49\\n64.11\\n57.12\\n67.17\\n60.72\\nViLT (Kim et al., 2021)\\ntrain-set\\n113M\\n60.48\\n63.89\\n60.27\\n63.20\\n61.38\\n57.00\\n60.72\\n61.90\\n61.14\\nPatch-TRM (Lu et al., 2021)\\ntrain-set\\n90M\\n65.19\\n46.79\\n65.55\\n66.96\\n55.28\\n64.95\\n58.04\\n67.50\\n61.42\\nVisualBERT (Li et al., 2019, 2020)\\ntrain-set\\n111M\\n59.33\\n69.18\\n61.18\\n62.71\\n62.17\\n58.54\\n62.96\\n59.92\\n61.87\\nText-to-text LLMs\\nUnifiedQAbase (Raffel et al., 2020b)\\nzero-shot\\n223M\\n68.16\\n69.18\\n74.91\\n63.78\\n61.38\\n77.84\\n72.98\\n65.00\\n70.12\\nGPT-3.5 (Chen et al., 2020)\\nzero-shot\\n175B\\n74.64\\n69.74\\n76.00\\n74.44\\n67.28\\n77.42\\n76.80\\n68.89\\n73.97\\nText-to-text LLMs with CoT\\nUnifiedQAbase (CoT) (Lu et al., 2022)\\nzero-shot\\n223M\\n71.00\\n76.04\\n78.91\\n66.42\\n66.53\\n81.81\\n77.06\\n68.82\\n74.11\\nGPT-3.5 (CoT) (Lu et al., 2022)\\n2-shot\\n175B\\n75.44\\n70.87\\n78.09\\n74.68\\n67.43\\n79.93\\n78.23\\n69.68\\n75.17\\nChatGPT (CoT) (Lu et al., 2023)\\nfew-shot\\n-\\n78.82\\n70.98\\n83.18\\n77.37\\n67.92\\n86.13\\n80.72\\n74.03\\n78.31\\nGPT-4 (CoT) (Lu et al., 2023)\\nfew-shot\\n-\\n85.48\\n72.44\\n90.27\\n82.65\\n71.49\\n92.89\\n86.66\\n79.04\\n83.99\\nMutimodal-CoTbase (Zhang et al., 2023)\\ntrain-set\\n223M\\n84.37\\n88.30\\n84.36\\n83.72\\n80.32\\n86.90\\n85.83\\n84.05\\n85.19\\nGoT-T5base\\ntrain-set\\n223M\\n86.25\\n93.55\\n85.51\\n85.89\\n86.30\\n86.34\\n87.79\\n87.23\\n87.59\\n± 0.31\\n± 0.06\\n± 0.11\\n± 0.32\\n± 0.28\\n± 0.12\\n± 0.10\\n± 0.40\\n± 0.20\\nMutimodal-CoTlarge (Zhang et al., 2023)\\ntrain-set\\n738M\\n91.03\\n93.70\\n86.64\\n90.13\\n88.25\\n89.48\\n91.12\\n89.26\\n90.45\\nGoT-T5large\\ntrain-set\\n738M\\n90.88\\n93.57\\n88.45\\n90.26\\n88.16\\n90.29\\n91.19\\n90.14\\n90.81\\n± 0.22\\n± 0.38\\n± 0.44\\n± 0.35\\n± 0.25\\n± 0.47\\n± 0.16\\n± 0.23\\n± 0.12\\nTable 2: Main test accuracy results (%) of ScienceQA. SIZE=backbone model size. Question classes: NAT =\\nnatural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO =\\nno context, G1-6 = grades 1-6, G7-12 = grades 7-12, AVG= average accuracy scores\\nswer generation stage, our GoT achieves SOTA in\\nall subjects and all grades. The most direct com-\\nparison is that our model achieves an accuracy of\\n87.59% which is 2.40% higher than that of the\\nMutimodal-CoTbase with the similar number of pa-\\nrameters.\\nGoT demonstrates a significant advantage over\\ntraditional CoT, elevating the accuracy from\\n30.09% to 32.09% in AQUA-RAT and from\\n85.19% to 87.59% in ScienceQA task. The results\\nsufficiently suggest that utilizing thought graph fea-\\ntures for deductive reasoning is a more effective\\napproach than the existing methods, which only\\nconsider text or vision features by simply incorpo-\\nrating image captions or fusing generated image\\nfeatures. In conclusion, our results confirm the\\neffectiveness of utilizing two-dimensional graph-\\nof-thought and demonstrate the potential of incor-\\nporating GoT into reasoning for LMs.\\n4.2\\nFurther Exploration\\n4.2.1\\nAblation Study\\nAQUA-RAT\\nIn order to make sure that intro-\\nducing thought graphs into GoT reasoning indeed\\nboost the performance, we conduct the following\\nexperiments:\\n(1) Random Thought Graph\\nIn the Random Thought Graph experiment, we\\nmaintain the GoT framework while introducing ran-\\ndomness into the process. We construct a thought\\ngraph by randomly selecting nodes and arbitrarily\\nestablishing connections between them. This ap-\\nproach is designed to evaluate the extent to which\\nthe GoT reasoning mechanism is reliant on the\\nstructured organization of thought graphs.\\n(2)\\nTriplets Concatenation In the Triplets Concate-\\nnation experiment, we take a straightforward ap-\\nproach by appending the extracted triplets directly\\nto the input text. This method aims to assess the\\nimpact of omitting the structural information typi-\\ncally provided by thought graphs, offering insight\\ninto the significance of this structural element in\\nthe reasoning process. (3) Coreference Injection\\nIn the Coreference Injection experiment, we ex-\\nplore the potential benefits of integrating corefer-\\nence resolution directly into the language model’s\\nreasoning process. We achieve this by incorpo-\\nrating coreference information into the input text,\\nwhere all instances of coreferent entities are re-\\nplaced with a consistent phrase, followed by model\\nfine-tuning. This experiment seeks to understand\\nthe role of coreference resolution in enhancing the\\nmodel’s deductive capabilities.\\nTable 3 shows the overall ablation results. From\\nthe table, we can see that by randomly construct\\nthought graphs to disrupt the deductive reasoning\\nprocess, our model suffers a loss of 1.78%, indicat-\\ning the effectiveness of GoT. The results of Triplets\\nMODEL\\nMODEL SIZE\\nACC\\n∆\\nGoT-T5base\\n233M\\n32.09\\n-\\nw/ Random Thought Graph\\n30.31\\n-1.78\\nTriplets Concatenation\\n233M\\n31.20\\n-0.89\\nCoreference Injection\\n233M\\n30.32\\n-1.77\\nTable 3:\\nAblation results of GoT on AQUA-RAT\\ndataset.\\nConcatenation on the AQUA-RAT showed an ac-\\ncuracy of 31.20%. This performance gap of 0.89\\nclearly demonstrates the significance of the struc-\\ntural information in our approach. For Coreference\\nInjection, the model suffers a loss of 1.77 % ac-\\ncuracy. We believe that these outcomes can be\\nattributed to a couple of factors: (1) Simply re-\\nplacing coreferent entities may lead to a loss of\\ncoherence in sentences, resulting in a reduction\\nof semantic information and consequently having\\na limited impact on overall accuracy. (2) Open\\nInformation Extraction (OpenIE) for coreference\\nresolution is not flawless, and direct replacement\\nof entities might introduce noise that misleads the\\nlanguage model during judgment.\\nContrastingly, the construction of a thought\\ngraph in the GoT framework does not compromise\\nthe original textual information (questions and ra-\\ntionales). Instead, it introduces additional structural\\nassistance for LMs to conduct reasoning effectively.\\nThus, we contend that GoT’s approach is indispens-\\nable and beneficial, as it supplements the LM’s\\ncomprehension without introducing potential noise\\nor loss of coherence in the input text.\\nScienceQA\\nTo examine the impact of different\\nbackbone and vision encoder configurations on the\\nGoT, we employed a distinct set of model settings.\\nMore specifically, we adopted the pre-trained T5\\ncheckpoint UnifiedQA (Khashabi et al., 2020) as\\nthe backbone model and utilized DETR (Carion\\net al., 2020) for the vision encoder, with results\\nillustrated in the Table 4. As shown, our GoT out-\\nperforms Mutimodal-CoT across various model\\nconfigurations. A comparison reveals that GoT\\ncan achieve greater improvements on smaller mod-\\nels. We believe the main reason is that when the\\nlanguage model is not as robust, or when employ-\\ning a relatively weaker vision encoder like DETR\\ncompared to ViT, GoT can leverage the inherent\\ninformation within the language to enhance per-\\nformance significantly. Additionally, to prove that\\nour GoT’s performance gain is not simply due to\\nan increase in parameters, we conducted an abla-\\ntion study. We expanded the parameter count of\\nMultimodal-CoTbase to match our 233M model\\nsize by adding two layers of MLP instead of one in\\nthe gated fusion module, referred to as Multimodal-\\nCoTbase(enlarged). We also constructed a random\\nthought graph ablation study on the ScienceQA\\ndataset. The results from the ablation studies can\\nbe observed in the table 4. From the table, it is\\nevident that our model significantly outperforms\\nthe enlarged Multimodal-CoT by an accuracy of\\n2.04%. These findings convincingly demonstrate\\nthe significance of incorporating thought graphs\\ninto multimodal reasoning. The performance of\\nGoT with a randomly constructed thought graph\\nwas even lower than Mutimodal-CoT, indicating\\nthat when the language model and vision encoder\\nare weaker, the model relies more heavily on GoT\\nfor reasoning.\\nModel\\nACC\\n∆\\nUnifiedQA+DETR\\nMutimodal-CoTbase\\n77.67\\n-\\nMutimodal-CoTlarge\\n81.37\\n-\\nGoTbase\\n81.21\\n3.54\\nGoTlarge\\n82.74\\n1.37\\nAblation Studies\\nMutimodal-CoTbase(enlarged)\\n79.17\\n-2.04\\nGoTbase w/ Random Thought Graph\\n76.74\\n-4.47\\nTable 4: Ablation results of GoT on ScienceQA dataset.\\nFor GoT models ∆indicates the performance gains of\\nGoT models over their Multimodal-CoT counterparts.\\nIn the ablation studies, ∆represents improvements rela-\\ntive to the GoTbase model\\n4.2.2\\nAnalysis\\nPerformance on Different Classes\\nIn order to\\ninvestigate the impact of GoT on the overall model\\nperformance across different subjects , we calcu-\\nlated the accuracy for different subjects and com-\\npared it with that of Mutimodal-CoT. We also\\ncompare the performance of two models on dif-\\nferent question classes.The radar Figure 5 shows\\nthe overall results for our base model. With re-\\nspect to various subjects and question classes, our\\nmodel demonstrates superior performance over the\\nMutimodal-CoTbase and attains a more consistent\\nand enhanced outcome. Our model presents out-\\nstanding advantages especially in the field of social\\nscience, with an accuracy improvement of 5.25%.\\nFor different question classes, our model demon-\\nstrates the largest improvement on questions involv-\\ning images. Our hypothesis is that by constructing\\na thought graph and integrating the three features of\\ntext, image, and thought graph, we can better align\\nthe textual and visual information for the model,\\nthus maximizing the utilization of visual informa-\\ntion and obtaining more accurate answers.\\nFigure 5: Performance on different question classes\\n2\\n4\\n6\\n8\\n10\\n12\\n60\\n70\\n80\\n90\\n100\\nGrades\\nAccuracy(%)\\nOursbase\\nMutimodal-CoTbase\\nFigure 6: Performance on different grades\\nPerformance on Different Grades\\nIt can be\\nseen from the Table 2 that Mutimodal-CoT expe-\\nrience a decrease in accuracy of 1.78 as the grade\\nlevel of the given question increases while GoT\\nonly has minor decrease of 0.56. We believe the\\nmain reason is that by incorporating GoT, models\\nacquires the ability for deductive reasoning and can\\nbetter comprehend the relationships between differ-\\nent entities and thus better understand the meaning\\nof the problems. Through this method, for higher-\\ngrade problems with greater complexity, the model\\ncan construct a thought graph to help itself gener-\\nate a more complete logical chain for deduction,\\nthereby generating more accurate answers. More\\ndetailed model performance on different grades can\\nbe found in Figure 6. We can see that in the lower\\ngrade, two models achieves a similar performance.\\nAs the grade level increases and the difficulty of\\nthe questions becomes more challenging, the gap\\nbetween our model and the Mutimodal-CoT model\\ngradually widens. Due to the small number of ques-\\ntions (≤130) available for each grade in grade 1\\nand grades 11-12, there is greater fluctuation in the\\naccuracy of both models. Nevertheless, it is evident\\nfrom the table that our model exhibits stronger and\\nmore stable advantages over Mutimodal-CoT in\\neach grade.\\nCase Study and Limitation\\nIn order to gain a\\ndeeper understanding of the performance of GoT,\\nwe conduct case studies which can be found in\\nthe Appendix E. We also visualize the attention\\nweights aij in GoT encoder to demonstrate how\\nGoT performs deductive reasoning to generate\\nmore accurate answers in Appendix F. For the lim-\\nitation of this work, compared to CoT, GoT may\\nresult in additional computational costs and slightly\\nslower training times. Detailed limitation analysis\\ncan be found in Appendix G.\\n5\\nConclusion\\nWe introduce a novel Graph-of-Thought (GoT) rea-\\nsoning approach, which is an innovative method\\nfor modeling the non-sequential nature of human\\nthinking for LMs. GoT enhances LMs with deduc-\\ntive reasoning abilities, providing a more realistic\\nrepresentation of thought processes. Our exper-\\niments showcases the superiority of GoT on the\\ntext-only reasoning dataset AQUA-RAT, achieving\\na similar result compared to GPT-3 model while\\nutilizing significantly fewer parameters. Further-\\nmore, GoT establishes a new state-of-the-art on\\nthe multimodal reasoning benchmark, ScienceQA\\nwith fewer parameters. This performance surpasses\\nstrong ChatGPT and GPT-4 systems, as well as hu-\\nman performance, demonstrating the efficacy of\\nGoT. Through comprehensive case studies and ab-\\nlation studies, we provide substantial evidence of\\nthe effectiveness of GoT in reasoning tasks. If you\\nwant it, you GoT it!\\nReferences\\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\\n2018. Bottom-up and top-down attention for image\\ncaptioning and visual question answering. In 2018\\nIEEE Conference on Computer Vision and Pattern\\nRecognition, CVPR 2018, Salt Lake City, UT, USA,\\nJune 18-22, 2018, pages 6077–6086. Computer Vi-\\nsion Foundation / IEEE Computer Society.\\nGabor Angeli, Melvin Jose Johnson Premkumar, and\\nChristopher D. Manning. 2015. Leveraging linguis-\\ntic structure for open domain information extraction.\\nIn Proceedings of the 53rd Annual Meeting of the As-\\nsociation for Computational Linguistics and the 7th\\nInternational Joint Conference on Natural Language\\nProcessing (Volume 1: Long Papers), pages 344–354,\\nBeijing, China. Association for Computational Lin-\\nguistics.\\nLawrence W Barsalou. 1999. Perceptual symbol sys-\\ntems. Behavioral and brain sciences, 22(4):577–660.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual.\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve,\\nNicolas Usunier, Alexander Kirillov, and Sergey\\nZagoruyko. 2020. End-to-end object detection with\\ntransformers. In Computer Vision - ECCV 2020 -\\n16th European Conference, Glasgow, UK, August\\n23-28, 2020, Proceedings, Part I, volume 12346 of\\nLecture Notes in Computer Science, pages 213–229.\\nSpringer.\\nJiaao Chen and Diyi Yang. 2021. Structure-aware ab-\\nstractive conversation summarization via discourse\\nand action graphs. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the Asso-\\nciation for Computational Linguistics: Human Lan-\\nguage Technologies, NAACL-HLT 2021, Online, June\\n6-11, 2021, pages 1380–1391. Association for Com-\\nputational Linguistics.\\nTing Chen, Simon Kornblith, Kevin Swersky, Moham-\\nmad Norouzi, and Geoffrey E. Hinton. 2020. Big\\nself-supervised models are strong semi-supervised\\nlearners. In Advances in Neural Information Pro-\\ncessing Systems 33: Annual Conference on Neural\\nInformation Processing Systems 2020, NeurIPS 2020,\\nDecember 6-12, 2020, virtual.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\\nRewon Child, Oleksandr Polozov, Katherine Lee,\\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\\nand Noah Fiedel. 2022. Palm: Scaling language mod-\\neling with pathways. CoRR, abs/2204.02311.\\nAlexey\\nDosovitskiy,\\nLucas\\nBeyer,\\nAlexander\\nKolesnikov,\\nDirk Weissenborn,\\nXiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\\nUszkoreit, and Neil Houlsby. 2021.\\nAn image\\nis worth 16x16 words:\\nTransformers for image\\nrecognition at scale. In 9th International Conference\\non Learning Representations, ICLR 2021, Virtual\\nEvent, Austria, May 3-7, 2021. OpenReview.net.\\nPeng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu,\\nSteven C. H. Hoi, Xiaogang Wang, and Hongsheng\\nLi. 2019.\\nDynamic fusion with intra- and inter-\\nmodality attention flow for visual question answer-\\ning. In IEEE Conference on Computer Vision and\\nPattern Recognition, CVPR 2019, Long Beach, CA,\\nUSA, June 16-20, 2019, pages 6639–6648. Computer\\nVision Foundation / IEEE.\\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei\\nCui, Owais Khan Mohammed, Barun Patra, Qiang\\nLiu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,\\nVishrav Chaudhary, Subhojit Som, Xia Song, and\\nFuru Wei. 2023.\\nLanguage is not all you need:\\nAligning perception with language models. CoRR,\\nabs/2302.14045.\\nMarek Kadlˇcík, Michal Štefánik, Ondrej Sotolar, and\\nVlastimil Martinek. 2023. Calc-X and calcformers:\\nEmpowering arithmetical chain-of-thought through\\ninteraction with symbolic systems. In Proceedings\\nof the 2023 Conference on Empirical Methods in\\nNatural Language Processing, pages 12101–12108,\\nSingapore. Association for Computational Linguis-\\ntics.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-\\nharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\\nHajishirzi. 2020. Unifiedqa: Crossing format bound-\\naries with a single QA system. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2020, Online Event, 16-20 November 2020, volume\\nEMNLP 2020 of Findings of ACL, pages 1896–1907.\\nAssociation for Computational Linguistics.\\nJin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.\\n2018. Bilinear attention networks. In Advances in\\nNeural Information Processing Systems 31: Annual\\nConference on Neural Information Processing Sys-\\ntems 2018, NeurIPS 2018, December 3-8, 2018, Mon-\\ntréal, Canada, pages 1571–1581.\\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:\\nVision-and-language transformer without convolu-\\ntion or region supervision. In Proceedings of the\\n38th International Conference on Machine Learning,\\nICML 2021, 18-24 July 2021, Virtual Event, volume\\n139 of Proceedings of Machine Learning Research,\\npages 5583–5594. PMLR.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022.\\nLarge\\nlanguage models are zero-shot reasoners.\\nCoRR,\\nabs/2205.11916.\\nBei Li, Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong\\nXiao, Anxiang Ma, and JingBo Zhu. 2022. On vi-\\nsion features in multimodal machine translation. In\\nProceedings of the 60th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 6327–6337, Dublin, Ireland. As-\\nsociation for Computational Linguistics.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\\nple and performant baseline for vision and language.\\nCoRR, abs/1908.03557.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\\nHsieh, and Kai-Wei Chang. 2020. What does BERT\\nwith vision look at?\\nIn Proceedings of the 58th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics, ACL 2020, Online, July 5-10, 2020,\\npages 5265–5275. Association for Computational\\nLinguistics.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\\nsom. 2017. Program induction by rationale genera-\\ntion: Learning to solve and explain algebraic word\\nproblems. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 158–167, Vancouver,\\nCanada. Association for Computational Linguistics.\\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-\\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\\nClark, and Ashwin Kalyan. 2022. Learn to explain:\\nMultimodal reasoning via thought chains for science\\nquestion answering. In The 36th Conference on Neu-\\nral Information Processing Systems (NeurIPS).\\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\\nsitional reasoning with large language models. CoRR,\\nabs/2304.09842.\\nPan Lu, Liang Qiu, Jiaqi Chen, Tanglin Xia, Yizhou\\nZhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and\\nSong-Chun Zhu. 2021. Iconqa: A new benchmark\\nfor abstract diagram understanding and visual lan-\\nguage reasoning. In Proceedings of the Neural In-\\nformation Processing Systems Track on Datasets and\\nBenchmarks 1, NeurIPS Datasets and Benchmarks\\n2021, December 2021, virtual.\\nChristopher Manning, Mihai Surdeanu, John Bauer,\\nJenny Finkel, Steven Bethard, and David McClosky.\\n2014. The Stanford CoreNLP natural language pro-\\ncessing toolkit. In Proceedings of 52nd Annual Meet-\\ning of the Association for Computational Linguis-\\ntics: System Demonstrations, pages 55–60, Balti-\\nmore, Maryland. Association for Computational Lin-\\nguistics.\\nOpenAI. 2023. Gpt-4 technical report.\\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,\\nShaohan Huang, Shuming Ma, and Furu Wei. 2023.\\nKosmos-2: Grounding multimodal large language\\nmodels to the world. CoRR, abs/2306.14824.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020a. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research,\\n21(140):1–67.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020b. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\\n2018. Graph attention networks. In 6th International\\nConference on Learning Representations, ICLR 2018,\\nVancouver, BC, Canada, April 30 - May 3, 2018,\\nConference Track Proceedings. OpenReview.net.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\\nLe, Ed H. Chi, and Denny Zhou. 2022.\\nSelf-\\nconsistency improves chain of thought reasoning in\\nlanguage models. CoRR, abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022a.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. CoRR, abs/2201.11903.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\\nand Denny Zhou. 2022b. Chain-of-thought prompt-\\ning elicits reasoning in large language models. In\\nAdvances in Neural Information Processing Systems\\n35: Annual Conference on Neural Information Pro-\\ncessing Systems 2022, NeurIPS 2022, New Orleans,\\nLA, USA, November 28 - December 9, 2022.\\nZhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, and\\nBen Kao. 2021. Good for misconceived reasons: An\\nempirical revisiting on the need for visual context\\nin multimodal machine translation. In Proceedings\\nof the 59th Annual Meeting of the Association for\\nComputational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing\\n(Volume 1: Long Papers), pages 6153–6166, Online.\\nAssociation for Computational Linguistics.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L Griffiths,\\nYuan Cao,\\nand Karthik\\nNarasimhan. 2023.\\nTree of thoughts: Deliberate\\nproblem solving with large language models. arXiv\\npreprint arXiv:2305.10601.\\nZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.\\n2019. Deep modular co-attention networks for visual\\nquestion answering. In IEEE Conference on Com-\\nputer Vision and Pattern Recognition, CVPR 2019,\\nLong Beach, CA, USA, June 16-20, 2019, pages 6281–\\n6290. Computer Vision Foundation / IEEE.\\nZhuosheng Zhang, Kehai Chen, Rui Wang, Masao\\nUtiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\\n2020.\\nNeural machine translation with universal\\nvisual representation. In 8th International Confer-\\nence on Learning Representations, ICLR 2020, Addis\\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\\nZhuosheng Zhang and Aston Zhang. 2023. You only\\nlook at screens: Multimodal chain-of-action agents.\\nCoRR, abs/2309.11436.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\\nSmola. 2022. Automatic chain of thought prompting\\nin large language models. CoRR, abs/2210.03493.\\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\\nGeorge Karypis, and Alex Smola. 2023.\\nMulti-\\nmodal chain-of-thought reasoning in language mod-\\nels. CoRR, abs/2302.00923.\\nAppendix\\nA\\nRelated Works\\nIn chain-of-thought reasoning, one idea leads to the\\nnext in a logical sequence and builds on previous\\nknowledge. Each idea is supported by evidence\\nor reasoning, and the conclusions drawn from the\\nchain are logical and sound. Most CoT methods\\ncan be divided into two categories based on how to\\ngenerate the final answer: (1) prompting for CoT,\\nincluding zero-shot CoT and few-shot CoT; and (2)\\nfine-tuning for CoT.\\nZero-shot CoT Prompting\\nAs large language\\nmodels continue to advance rapidly, many re-\\nsearchers are beginning to explore CoT reasoning\\nfor LLMs. The zero-shot CoT method proposed\\nby Kojima et al. (2022) consists of two stages: (1)\\nadding a \"Let’s think step by step\" prompt to gener-\\nate CoT, and (2) concatenating the generated CoT\\nand adding the phrase \"So the answer is\" to ob-\\ntain the final answer. Tree-of-Thought (ToT) (Yao\\net al., 2023) enables deliberate decision-making\\nthrough exploration of coherent text units. ToT di-\\nvides thoughts into thought units and models them\\nas a tree-like search process. Although both GoT\\nand ToT aim to capture human non-linear thoughts,\\nGoT is distinct from ToT in terms of both methodol-\\nogy and objectives. We believe that human thinking\\ninvolves both linear and non-linear aspects. Thus,\\nwe build upon the linear CoT framework by in-\\ncorporating non-linear structures to simultaneously\\ncapture both linear and non-linear human reason-\\ning. Tree-of-thoughts focuses on modeling non-\\nlinear thoughts explicitly, whereas our approach\\nleverages non-linear structures to assist the Chain-\\nof-Thought reasoning.\\nFew-shot CoT Prompting\\nFew-shot CoT rea-\\nsoning for LLMs, however, utilizes multiple input-\\noutput pairs to prompt the LLMs to output CoT\\nand obtain the final answer. Due to its ability to\\nprovide better performance compared to Zero-shot\\nCoT, Few-shot CoT has gained more attention in\\nresearch, particularly through effective demonstra-\\ntions. Few-shot CoT prompting was first formally\\nexplored by Wei et al. (2022a) and is a form of dis-\\ncrete prompt learning that involves context learning\\nin large models. Compared to traditional in-context\\nlearning, which prompts LLMs with a list of input-\\noutput demonstration pairs along with a test input\\nto allow the model to predict output, Few-shot CoT\\nprompting outputs additional logical reasoning pro-\\ncedures apart from the target output. Wang et al.\\n(2022) proposed a follow-up method to (Wei et al.,\\n2022a). The main improvement is that the model\\nuses the majority vote for the answers, which was\\nfound to significantly improve the performance of\\nthe CoT. However, these few-shot CoT models de-\\npend on hand-crafted demonstrations. To solve this\\nproblem, Zhang et al. (2022) proposed Auto-CoT,\\nwhich maintains the diversity of sampled questions\\nand generates reasoning chains to automatically\\nconstruct demonstrations. Specifically, Auto-CoT\\nconsists of two main stages: (1) Problem clustering:\\ndivide the given dataset of problems into several\\nclusters; (2) Demonstration sampling: select a rep-\\nresentative problem from each cluster and use a\\nsimple heuristic method to generate its reasoning\\nchain. Furthermore, Lu et al. (2023) also explores\\nfew-shot CoT reasoning for recently popular LLMs\\nChatGPT and GPT-4.\\nCoT Fine-tuning\\nIn Zhang et al. (2023), it was\\nproposed to fine-tune smaller language models in-\\nstead of prompting them in LLMs. And this ap-\\nproach enabled the CoT to go beyond textual infor-\\nmation and incorporate visual (image) modalities\\nusing a gated fusion mechanism into a two-stage\\nCoT. The results demonstrated that CoT fine-tuning\\nwith fewer parameters has potential. Therefore, in\\nthis work, we focus on fine-tuning for CoT to re-\\nduce the number of required model parameters and\\nhelp LLMs better comprehend different modalities.\\nHowever, previous CoT research has been limited\\nto different modalities, such as textual and vision\\ninformation, without considering the deduction rea-\\nsoning process. Therefore, in this work, we move\\nbeyond modeling the reasoning process solely as\\na thought chain and elevate it to a thought graph.\\nWe provide a more comprehensive and nuanced\\nrepresentation, enabling LLMs to perceive the de-\\nduction reasoning process accurately, resulting in\\nmore precise answer generation.\\nB\\nDataset\\nAQUA-RAT dataset consists of about 100,000 al-\\ngebraic word problems with natural language ra-\\ntionales. For AQUA-RAT, the model is trained to\\nreasoning through the steps to generate the final\\nanswer. ScienceQA benchmark is the pioneering\\nlarge-scale dataset for multimodal science ques-\\ntions, equipped with comprehensive annotations for\\nanswers, including detailed lectures and explana-\\ntions. The dataset contains 21k questions covering\\nthree subjects: natural science, language science,\\nand social science. Each question is presented with\\na context in the form of natural language or an\\noptional image. The model is trained to elucidate\\nthe reasoning process in natural language while\\nchoosing the answer from a set of options.\\nSplits\\n#Problems\\nTrain\\n97467\\nDev\\n254\\nTest\\n254\\nTable 5: AQUA-RAT dataset statistics (# denotes num-\\nbers)\\nStatistic\\nNumber\\nSplits\\n#Train\\n12,726\\n#Dev\\n4,241\\n#Test\\n4,241\\n#Total\\n21,208\\nAttribute\\n#Subjects\\n3\\n#Topic\\n26\\n#Category\\n127\\n#Skill\\n379\\nTable 6: ScienceQA dataset statistics (# denotes num-\\nbers)\\nC\\nTraining Parameters\\nParameters\\nValue\\nEpochs\\n100\\nBatch size for T5-base (per device)\\n10\\nBatch size for T5-large (per device)\\n8\\nLearning rate\\n5e-5\\nWeight decay\\n0.01\\nMax input length\\n512\\nMax number of nodes\\n150\\nTable 7: Training parameters for GoT\\nD\\nRationale Generation Results\\nThe rationale genration results can be found in Ta-\\nble 8. We can observe from Table 8 that the im-\\npact of GoT on rationale generation is limited. We\\nattribute this limitation to the fact that the input\\ntext for thought graph construction only includes\\nquestions and choices. Consequently, the thought\\ngraph constructed from such limited information\\ncan only facilitate constrained deductive reasoning.\\nHowever, in the answer generation stage, when pro-\\nvided with rationales, the model needs to possess\\nstronger deductive reasoning capabilities to under-\\nstand the relationship between rationales, questions,\\nand choices.\\nE\\nCase Study\\nTo facilitate a more illustrative comparison between\\nGoT and the CoT, we have selected several repre-\\nsentative examples. Figure 7 illustrates the exam-\\nples from AQUA-RAT dataset. Figure 8 to Figure\\n11 illustrates examples from ScienceQA dataset.\\nFrom Figure 8 and Figure 9, we can see that GoT\\ncan better understand the rationales and generate\\nmore accurate result. In Figure 10, we can see that\\nwhen provided with wrong rationale, our model is\\nmore robust to the noise and can focus on more\\nimportant key information. (We highlight the noisy\\nwrong rationale in red and correct key rationale\\nin green). Figure 11 presents a language prob-\\nlem which have less context and requires a certain\\namount of common sense knowledge. Hence, the\\nimpact of constructing a mind map on enhancing\\nthe model is not significant. Therefore, both GoT\\nand CoT predict wrong answers.\\nF\\nRepresentation Visualization\\nIn order to demonstrate the deductive reasoning\\nprocess of GoT more intuitively, we visualized the\\nattention weights of the GoT encoder. The visu-\\nalization results can be found in Figure 12. We\\ntook Figure 10 as an example. In Figure 10, even\\ngiven a wrong rationale, GoT still manages to gen-\\nerate the right answer. We select 14 representative\\nthought nodes and found that \"blue\",\"color\", and\\n\"common\" have the greatest weights which indi-\\ncates that GoT guides the model to focus on more\\nimportant words and conduct correct deductive rea-\\nsoning. For the disruptive node \"a hard object,\" our\\nmodel can effectively discriminate against it and as-\\nsign a lower attention weight to prevent the model\\nfrom selecting incorrect answers, as traditional CoT\\nmodels often do due to erroneous rationales.\\nG\\nLimitation\\nCompared to Mutimodal-CoT (Zhang et al., 2023),\\nincorporating GoT may result in additional com-\\nputational costs and slightly slower training times.\\nMODELS\\nBLEU1\\nBLEU4\\nROUGE\\nSIMILARITY\\nAQUA-RAT\\nFLAN-Alpacabase\\n19.78\\n3.49\\n28.40\\n68.61\\nFLAN-Alpacalarge\\n22.45\\n5.40\\n29.55\\n70.34\\nGoT-T5base\\n22.05\\n5.02\\n29.18\\n69.09\\nGoT-T5large\\n24.47\\n6.68\\n29.86\\n71.58\\nScienceQA\\nMutimodal-CoT∗\\nbase (Zhang et al., 2023)\\n91.04\\n86.81\\n93.24\\n96.34\\nGoT-T5base\\n92.50\\n88.79\\n94.39\\n96.74\\nGoT-T5large\\n93.49\\n90.09\\n95.17\\n97.33\\nTable 8: Rationale generation results (%). (*: we re-run the Mutimodal-CoTbase to report the full rationale scores.\\nWe use sentence-transformers (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to obtain sentence\\nembeddings and calculate the cosine similarity for SIMILARITY)\\nThe training parameters and inference times of the\\ndifferent models are presented in Table 9, which\\nreveals that our model requires a 0.2% increase in\\nparameters compared to Mutimodal-CoT.\\n#Parameters\\nInference time\\n(eval samples/per second)\\nMutimodal-CoTbase\\n227M\\n16.33\\nOurs\\n233M\\n13.38\\nTable 9: The number of training parameters and infer-\\nence time of different models (# denotes numbers)\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: paper is in a square form whose one side is 20 cm. Two semi circles \\nare drawn on its opposites as diameters. If these semi circles are cut down \\nwhat is the area of the remaining paper\\nChoices:\\n(A) 8.73\\n(B) 8.79\\n(C) 8.75\\nRationale: (5 * 3.5)/2 = 8.75\\nAnswer: C\\nRationale: Area of the square = pi*r2/4 = (20*20)/(2*2) = 8.73\\n  \\nAnswer: A\\nRationale: Explanation: Area of the paper = (r2 + r2) = (20 x 20) cm2 = 1200 cm2. Area of \\ntwo semi circles cut down = [(20 - r2)2 - (20 x 20)2] cm2 = (1200 - 225) cm2 = 875 cm2.\\nAnswer: C\\n(D) 8.71\\nwrong rationales right answer\\nwrong rationales wrong answer\\n(E) 8.72\\nFigure 7: Examples of AQUA-RAT\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: Would you find the word pink on a dictionary page with the following guide words?\\nparrot – property\\nChoices:\\n(A) yes\\n(B) no\\nRationale: lecture: Guide words appear on each page of a dictionary. They tell you the first\\nword and last word on the page. The other words on the page come between the guide\\nwords in alphabetical order. To put words in alphabetical order, put them in order by their\\nfirst letters. If the first letters are the same, look at the second letters. If the second letters\\nare the same, look at the third letters, and so on. If one word is shorter, and there are no\\nmore letters to compare, then the shorter word comes first in alphabetical order. For\\nexample, be comes before bed.\\nsolution: Put the words in alphabetical order. Since pink is between the guide words parrot -\\nproperty, it would be found on that page.\\nAnswer: The answer is (A)\\nRationale: Solution: Guide words appear on each page of a dictionary. They tell you the first\\nword and last word on the page. The other words on the page come between the guide\\nwords in alphabetical order. To put words in alphabetical order, put them in order by their\\nfirst letters. If the first letters are the same, look at the second letters. If the second letters\\nare the same, look at the third letters, and so on. If one word is shorter, and there are no\\nmore letters to compare, then the shorter word comes first in alphabetical order. For\\nexample, be comes before bed. Put the words in alphabetical order. Since pink is between\\nthe guide words parrot - property, it would be found on that page.\\nAnswer: The answer is (A)\\nRationale: Solution: Guide words appear on each page of a dictionary. They tell you the first\\nword and last word on the page. The other words on the page come between the guide\\nwords in alphabetical order. To put words in alphabetical order, put them in order by their\\nfirst letters. If the first letters are the same, look at the second letters. If the second letters\\nare the same, look at the third letters, and so on. If one word is shorter, and there are no\\nmore letters to compare, then the shorter word comes first in alphabetical order. For\\nexample, be comes before bed. Put the words in alphabetical order. Since pink is between\\nthe guide words parrot - property, it would be found on that page.\\nAnswer: The answer is (B)\\nright rationales right answer\\nright rationales wrong answer\\nFigure 8: Examples of ScienceQA\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: What is the name of the colony shown?\\nChoices:\\n(A) New Hampshire\\n(B) Vermont\\n(C) New York\\nRationale: The colony is New York. During the\\ncolonial era, New Hampshire and New York both\\nclaimed the territory that would later become the\\nstate of Vermont. Vermont was never its own\\ncolony.\\nAnswer: The answer is (C)\\nRationale: Solution: The colony is New York. During the colonial era, New Hampshire and\\nNew York both claimed the territory that would later become the state of Vermont.\\nVermont was never its own colony.\\nAnswer: The answer is (C)\\nRationale: Solution: The colony is Delaware York. During the colonial era, New Hampshire\\nand New York both claimed the territory that would later become the state of Vermont.\\nVermont was never its own colony.\\nAnswer: The answer is (D)\\n(D) Delaware\\nright rationales right answer\\nwrong rationales wrong answer\\nFigure 9: Examples of ScienceQA\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: Which property do these three \\nobjects have in common?\\nChoices:\\n(A) blue\\n(B) hard\\n(C) sticky\\nRationale: lecture: An object has different properties. A property of an object can tell you\\nhow it looks, feels, tastes, or smells. Properties can also tell you how an object will behave\\nwhen something happens to it. Different objects can have properties in common. You can\\nuse these properties to put objects into groups.\\nSolution: Look at each object. For each object, decide if it has that property. A sticky object\\ncan attach or stick to other things. The toothpaste is sticky, but the soccer shorts and the\\nwater slide are not. Blue is a color. This color is blue. All three objects are blue. A hard object\\ndoes not change shape when pressed or squeezed. The water slide is hard, but the\\ntoothpaste and the soccer shorts are not. The property that all three objects have in\\ncommon is blue.\\nAnswer: The answer is (A)\\nRationale: Solution: An object has different properties. A property of an object can tell you\\nhow it looks, feels, tastes, or smells. Properties can also tell you how an object will behave\\nwhen something happens to it. Different objects can have properties in common. You can\\nuse these properties to put objects into groups. Look at each object. For each object, decide\\nif it has that property. A hard object can attach or stick to other things. The is sticky, but the\\nshorts and the pitcher are not. Blue is a color. This color is blue. The three objects are blue.\\nA hard object does not change shape when pressed or squeezed. The tennis slide and hard,\\nbut the tennis and the water shorts are not. The property that all three objects have in\\ncommon is blue.\\nAnswer: The answer is (A)\\nRationale: Solution: An object has different properties. A property of an object can tell you\\nhow it looks, feels, tastes, or smells. Properties can also tell you how an object will behave\\nwhen something happens to it. Different objects can have properties in common. You can\\nuse these properties to put objects into groups. Look at each object. For each object, decide\\nif it has that property. A sticky object can attach or stick to other things. The is sticky, but the\\nshorts and the blue bottle are not. Blue is a color. This color is blue. None three objects are\\nblue. A hard object does not change shape when pressed or squeezed. None tennis slide\\nand hard, but the is the water shorts are not. The property that all three objects have in\\ncommon is sticky.\\nAnswer: The answer is (C)\\nwrong rationales right answer\\nwrong rationales wrong answer\\nFigure 10: Examples of ScienceQA\\nDataset\\nGoT Prediction\\nCoT Prediction\\nQuestion: Select the action that doesn\\'t belong\\nChoices:\\n(A) chop\\n(B) blend\\n(C) stir\\nRationale: Chop doesn\\'t belong. Blend, mix, and stir all describe ways to combine\\nthings\\nAnswer: The answer is (A)\\nRationale: Solution: Mixp doesn‘t belong. Murend, chop, and chop all name things to get\\nthings.\\nAnswer: The answer is (D)\\nRationale: Solution: Blendp doesn\\'t belong..Murend, chop, and blend all name things to\\ngetAnswer: The answer is (B)\\n(D) mix\\nwrong rationales wrong answer\\nwrong rationales wrong answer\\nFigure 11: Examples of ScienceQA\\nthree objects\\nhave in\\ncommon\\nobject\\nhas\\ndifferent properties\\nput objects into\\ngroups\\na hard object\\ncan attach to\\nother things\\nis\\ncolor\\nblue\\n49.56\\n44.00\\nFigure 12: Representation visualization\\n'),\n",
       " SearchResult(title='Table as Thought: Exploring Structured Thoughts in LLM Reasoning', published='2025-01-04', summary=\"Large language models' reasoning abilities benefit from methods that organize\\ntheir thought processes, such as chain-of-thought prompting, which employs a\\nsequential structure to guide the reasoning process step-by-step. However,\\nexisting approaches focus primarily on organizing the sequence of thoughts,\\nleaving structure in individual thought steps underexplored. To address this\\ngap, we propose Table as Thought, a framework inspired by cognitive\\nneuroscience theories on human thought. Table as Thought organizes reasoning\\nwithin a tabular schema, where rows represent sequential thought steps and\\ncolumns capture critical constraints and contextual information to enhance\\nreasoning. The reasoning process iteratively populates the table until\\nself-verification ensures completeness and correctness. Our experiments show\\nthat Table as Thought excels in planning tasks and demonstrates a strong\\npotential for enhancing LLM performance in mathematical reasoning compared to\\nunstructured thought baselines. This work provides a novel exploration of\\nrefining thought representation within LLMs, paving the way for advancements in\\nreasoning and AI cognition.\", authors=['Zhenjie Sun', ' Naihao Deng', ' Haofei Yu', ' Jiaxuan You'], page_content='Table as Thought: Exploring Structured Thoughts in LLM Reasoning\\nZhenjie Sun1,2, Naihao Deng2, Haofei Yu1, Jiaxuan You1\\n1University of Illinois Urbana-Champaign, 2University of Michigan\\nAbstract\\nLarge language models’ reasoning abilities ben-\\nefit from methods that organize their thought\\nprocesses, such as chain-of-thought prompt-\\ning, which employs a sequential structure to\\nguide the reasoning process step-by-step. How-\\never, existing approaches focus primarily on\\norganizing the sequence of thoughts, leaving\\nstructure in individual thought steps underex-\\nplored. To address this gap, we propose Table\\nas Thought, a framework inspired by cognitive\\nneuroscience theories on human thought. Ta-\\nble as Thought organizes reasoning within a\\ntabular schema, where rows represent sequen-\\ntial thought steps and columns capture critical\\nconstraints and contextual information to en-\\nhance reasoning. The reasoning process itera-\\ntively populates the table until self-verification\\nensures completeness and correctness. Our ex-\\nperiments show that Table as Thought excels in\\nplanning tasks and demonstrates a strong poten-\\ntial for enhancing LLM performance in math-\\nematical reasoning compared to unstructured\\nthought baselines. This work provides a novel\\nexploration of refining thought representation\\nwithin LLMs, paving the way for advancements\\nin reasoning and AI cognition.\\n1\\nIntroduction\\nRecent advancements in reasoning have demon-\\nstrated that the reasoning capabilities of large lan-\\nguage models (LLMs) can be enhanced by intro-\\nducing structure into the reasoning process (Wei\\net al., 2023; Yao et al., 2023; Besta et al., 2024). For\\ninstance, the chain-of-thought approach organizes\\ntextual reasoning in a step-by-step manner using a\\nlinear chain structure (Wei et al., 2023). Building\\non this, following works have shown that incor-\\nporating more complex organizational structures\\nfurther improves reasoning performance (Besta\\net al., 2024; Yao et al., 2023). However, these\\napproaches structure reasoning only at the level\\nof connections between distinct reasoning steps\\n(inter-thought level) and leave the content of in-\\ndividual steps (thought level) unstructured. This\\nraises the critical question: Can LLMs’ reason-\\ning abilities be further enhanced by introducing\\nstructure within individual thoughts?\\nTo address this question, we draw inspiration\\nfrom cognitive neuroscience theories of human\\nthought. Neuroscientists have found that humans\\nthink in a structured way, with the brain’s orga-\\nnization facilitating sequential and goal-oriented\\nreasoning. Christoff and Gabrieli (2000) provided\\nearly evidence that the prefrontal cortex supports\\nstructured reasoning through a rostrocaudal hi-\\nerarchy, enabling the processing of increasingly\\nabstract concepts and complex goal-directed be-\\nhavior. Later, Friston (2005)’s predictive coding\\nframework demonstrated how structured cognition\\nemerges from the brain’s ability to build hierarchi-\\ncal models, combining experiences with current in-\\nput to predict results. More recently, Jeff Hawkins\\n(Hawkins, 2021) argued that humans think in a\\nstructured manner, with the neocortex organizing\\nknowledge in certain structures, and thinking arises\\nfrom neurons activating sequential locations in\\nthese frames. Building on these insights, we pro-\\npose investigating whether similarly structured rep-\\nresentations can be incorporated into LLMs to en-\\nhance their reasoning and planning capabilities.\\nIn this work, we adopt a simple yet effective\\nstructural format—a tabular schema—to approxi-\\nmate the structured nature of human thinking pro-\\ncesses. In our approach, the schema of a table\\nserves as a framework for organizing and navigat-\\ning knowledge. Inspired by the sequential pro-\\ncesses described in neuroscience—where neurons\\nactivate specific patterns step by step (Hawkins,\\n2021)—we model these processes as the sequen-\\ntial population of rows in a table, moving across\\ncolumns according to a predefined schema. A sin-\\ngle table can encapsulate one or more such struc-\\ntured thought processes, providing a coherent con-\\narXiv:2501.02152v1  [cs.AI]  4 Jan 2025\\ntainer for organizing and connecting thinking steps\\nand associated information. Tables not only repre-\\nsent step-by-step processes for achieving specific\\ngoals but also serve as robust frameworks for plan-\\nning tasks. Moreover, utilizing tables as structured\\nrepresentations enables schema design that ensures\\norganization and data integrity, thereby facilitating\\nefficient verification and analysis.\\nThe contributions of our paper are as follows:\\n• Motivated by insights from cognitive neuro-\\nscience regarding the structured nature of hu-\\nman thinking, we propose a novel framework,\\nTable as Thought, that injects structure at the\\nthought level. To the best of our knowledge,\\nthis is the first exploration and demonstration\\nof integrating structured representations di-\\nrectly into the reasoning process of large lan-\\nguage models.\\n• We demonstrate the advantages of Table as\\nThought in tasks requiring planning and math-\\nematical reasoning, highlighting its poten-\\ntial to enhance performance on tasks that de-\\nmand sequential and goal-oriented thought\\nprocesses.\\n• We provide a detailed and comprehensive anal-\\nysis of Table as Thought, offering insights into\\nits functionality and strengths, and comparing\\nthe benefits of structured versus unstructured\\nthought representations. We believe these find-\\nings can inspire future research into the nature\\nand representation of thought processes in ar-\\ntificial intelligence and computational linguis-\\ntics.\\n2\\nRelated Work\\nStructures in LLM Reasoning\\nRecent advance-\\nments in large language models (LLMs) have in-\\ncreasingly focused on integrating structured pro-\\ncesses to enhance reasoning capabilities. Chain-of-\\nThought prompting (Wei et al., 2023) introduces a\\nstep-by-step framework that organizes thoughts in\\na sequential manner, enabling more coherent rea-\\nsoning. Building on this, Tree of Thoughts (Yao\\net al., 2023) and Graph of Thoughts (Besta et al.,\\n2024) employ hierarchical and networked struc-\\ntures to further enhance problem-solving, leverag-\\ning branching and interconnected paths. Moreover,\\nself-consistency (Wang et al., 2023) improves reli-\\nability by sampling multiple reasoning paths and\\nselecting the most consistent outcome, thereby ad-\\ndressing variability in generated responses.\\nWhile these methods excel at organizing rea-\\nsoning at a macro level—such as through chain-\\ning, branching, or aggregating thought paths—they\\ndo not address the internal structure of individual\\nthoughts. Our work is distinct in that it introduces\\nstructure directly at the thought level, refining the\\ngranularity of reasoning processes in LLMs. By\\nfocusing on the internal organization of individual\\nreasoning steps, we provide a novel perspective\\non enhancing the depth and precision of structured\\nreasoning in LLMs.\\nRepresentations of Tables in LLM Inference\\nTables have traditionally played a significant role\\nin LLMs for tasks involving the understanding\\nand processing of tabular data, such as knowledge\\nretrieval (Cong et al., 2024), question answering\\nover structured data (Yin et al., 2020; Zhang et al.,\\n2024b), and tabular reasoning (Herzig et al., 2020;\\nDeng et al., 2024). In these tasks, tables are lever-\\naged only as input for interpretation and manipula-\\ntion.\\nThe Chain-of-Table framework (Wang et al.,\\n2024) extends the application of tables by employ-\\ning them as proxies for intermediate thoughts in rea-\\nsoning tasks involving tabular data. In this frame-\\nwork, LLMs iteratively update a table, forming a\\ndynamic reasoning chain where the table evolves\\nbased on intermediate results. While this approach\\nhas proven effective on tabular-specific datasets, it\\nremains inherently tied to tasks where tabular data\\nis part of the input or reasoning context.\\nIn contrast, our work redefines the role of tables\\nby utilizing them as a universal framework for struc-\\nturing and representing the internal thought pro-\\ncesses of LLMs in non-table-specific tasks, such as\\nplanning and mathematical reasoning. Unlike prior\\napproaches that depend on pre-existing tabular in-\\nputs, we employ tables as dynamic containers to or-\\nganize and manipulate thoughts step by step. This\\napproach enables structured reasoning even in tasks\\nwhere no tabular data is initially present, bridging\\nthe gap between unstructured text-based reasoning\\nand structured problem-solving paradigms. By gen-\\neralizing the utility of tables beyond table-specific\\nreasoning tasks, our work marks a significant de-\\nparture from previous methods and demonstrates\\nthe versatility of this novel framework.\\nQuery\\nSchema Design\\nStructured \\nThoughts\\nReasoning Table\\nReasoning \\nVerification\\n{                \\nParticipant_Name: \\nMichelle,\\nUnavailable_Time:  \\n11:00 to 12:00,\\n...\\n }\\nParticipant Name\\nUnavailable Time\\n…\\nAvailable Time\\nSelected Time\\nParticipant\\nUnavailable \\n…\\nSelected \\nMichelle\\n11:00 to 12:00\\n9:30 to 10:30\\nSteven\\n9:00 to 9:30, 11:30 \\nto 12:00, ….\\n9:30 to 10:30\\nJerry\\n9:00 to 9:30, 10:00 \\nto 11:00, …\\n9:30 to 10:30\\nYou need to \\nschedule a \\nmeeting for \\nMichelle, \\nSteven and \\nJerry for \\none hour …\\n❌\\n✅\\nAnswer\\n✅: The reasoning \\nprocess is correct.\\n❌: Selected Time \\nconflicts with Jerry’s \\nunavailable time…\\nThe meeting \\nshould be \\nscheduled \\n9:30 to 10:30\\nFigure 1: The Overall Pipeline for Table as Thought Reasoning. The figure illustrates how Table as Thought\\nstructures reasoning by iteratively populating a reasoning table based on the schema, verifying consistency, and\\nupdating the table until the final answer is achieved.\\n3\\nTable as Thought\\nWe\\npresent\\nthe\\ndesign\\nof\\nthe\\nTable\\nas\\nThought framework, which introduces a novel\\napproach to reasoning in large language models by\\nleveraging tables as structured representations of\\nthoughts.\\nTable as Thought.\\nTable as Thought employs\\na table as a container to represent one or more\\nstructured thoughts. These tables, referred to as\\n\"reasoning tables\", encapsulate thoughts and pro-\\nvide a transparent representation of the reasoning\\nprocess. A reasoning table T is initialized with an\\noriginal table schema S, which is defined by the\\nLLM for a given query Q. Structured thoughts Θ\\nare then generated based on S, with each thought\\ncorresponding to a row in the reasoning table T.\\nThe table T is subsequently populated and updated\\naccording to these structured thoughts Θ.\\nThe overall reasoning workflow using the reason-\\ning table is illustrated in Figure 1 and formalized\\nin Algorithm 1.\\nAlgorithm 1 Table as Thought\\nRequire: Query Q\\nEnsure: A table T that satisfies Q\\n1: S ←DESIGNSCHEMA(Q)\\n// Define table\\nschema\\n2: Initialize an empty table T with schema S.\\n3: while not SUFFICIENT(T, Q) do\\n4:\\nΘ ←REFLECT(T, Q) // Generate possible\\nupdates\\n5:\\nT ←UPDATETABLE(T, Θ)\\n// Apply\\nupdates if needed\\n6: end while\\n7: return T\\nSchema Development Module\\nThe Schema\\nDevelopment Module dynamically adapts table\\nschemas to accommodate various queries across\\ndifferent reasoning tasks. For constraint-planning\\ntasks, where the primary objective is to satisfy\\nconstraints, we prompt LLMs to identify the con-\\nstraints explicitly before designing the schema.\\nThis ensures that both explicit and implicit con-\\nstraints are addressed in the reasoning process. For\\nmathematical reasoning tasks, the schema is tai-\\nlored to reflect the logical progression of the rea-\\nsoning steps, enabling systematic organization of\\ncritical information.\\nThe headers in the table schemas are designed\\nto represent essential reasoning steps and key infor-\\nmation pertinent to the task. These headers act as\\nanchors for organizing and verifying intermediate\\nand final reasoning outputs.\\nFor example, consider the travel planning query:\\nI plan to travel alone, and my\\nplanned budget for the trip is\\naround $1,100.\\nIn this case, a key constraint is that the total cost\\nshould not exceed $1,400. To address this con-\\nstraint, the schema must include a header such as\\nCost, with the type Number, ensuring that the rele-\\nvant information is captured and evaluated against\\nthe budgetary constraint.\\nFor a mathematical reasoning task, such as a\\nquestion from the GSM8K dataset:\\nA robe takes 2 bolts of blue fiber\\nand half that much white fiber.\\nHow many bolts in total does it\\ntake?\\nHere, the reasoning process requires consideration\\nof the quantities of blue and white fibers. The\\nschema should therefore include keys such as Blue\\nFiber and White Fiber, ensuring that all relevant\\nelements are systematically tracked and calculated.\\nReasoning Verification Module\\nThe inclusion\\nof this module stems from our findings during ex-\\nperiments that current LLMs sometimes fail to gen-\\nerate the complete reasoning process with struc-\\ntured thoughts to solve a query. However, this\\nmodule is designed not only to verify the complete-\\nness of the reasoning process, but also to ensure its\\ncorrectness.\\nFor constraint reasoning tasks, the module guar-\\nantees that all necessary information required to\\nmeet the constraints defined in the schema is cap-\\ntured and satisfied. Specifically, it verifies whether\\nthe constraints identified during the schema devel-\\nopment phase are adhered to. Constraint check-\\ning is generally performed internally by the LLM\\nthrough reflective reasoning on the generated ta-\\nble, with constraints explicitly listed for verifica-\\ntion. The structured nature of thoughts in Table as\\nThought introduces an additional capability: Auto-\\nCheck Constraints, which are constraints set for\\nexternal verification, performed entirely by the sys-\\ntem to ensure that the table adheres to the defined\\nconstraints. By leveraging the structured repre-\\nsentations of Table as Thought, Auto-Check Con-\\nstraints facilitate the systematic validation of inter-\\nmediate steps and final outputs without relying on\\nthe LLM.\\nFor mathematical reasoning tasks, the module\\nevaluates the correctness of the reasoning process\\nby ensuring that the table reflects an accurate and\\nlogical reasoning path toward solving the problem.\\nThis involves checking whether the intermediate\\nand final outputs align with the expected reasoning\\nsteps outlined in the schema.\\nTable Construction Module\\nThe Table Con-\\nstruction Module iteratively generates structured\\nthoughts and constructs the reasoning table by in-\\ncorporating the schema and feedback from the rea-\\nsoning verification module. This process involves\\ndynamically adding new thoughts to the table, mod-\\nifying existing entries, or removing entries that do\\nnot align with the schema or query requirements.\\nThe iterative process terminates under one of the\\nfollowing conditions:\\n1. The reasoning table is verified as complete and\\ncorrect by the reasoning verification module.\\n2. The maximum number of iterations, which is\\n10 in all our experiments, is reached.\\n4\\nExperiments\\n4.1\\nTasks and Language Models\\nFor all tasks, we adopt the original evaluation meth-\\nods to ensure consistency and comparability.\\nConstraint Planning Tasks.\\nThe goal of con-\\nstraint planning tasks is to generate plans that sat-\\nisfy both explicit and implicit constraints. We eval-\\nuate our approach on two datasets, each presenting\\ndifferent levels of complexity in the expected plans.\\nThe TravelPlanner dataset (Xie et al., 2024) re-\\nquires LLMs to generate detailed travel plans that\\nadhere to explicit constraints provided in the query,\\nsuch as budget limitations, as well as implicit con-\\nstraints derived from common sense. The expected\\ntravel plans are highly complex, encompassing\\nmulti-day agendas that include transportation, ac-\\ncommodations, and daily attractions. Due to the\\nexceptionally long context required for this task,\\nwhich results in substantial token costs, we con-\\nduct experiments exclusively with GPT-4-o-mini.\\nThe calendar scheduling task from the NaturalPlan\\nbenchmark (Zheng et al., 2024) focuses on gener-\\nating single-object plans. In this task, LLMs must\\ndetermine an appropriate meeting time based on\\nexplicit constraints, such as the company’s work-\\ning hours and the unavailable time slots of each\\nparticipant.\\nMath reasoning tasks.\\nWe evaluate LLMs using\\nGSM-8K and MATH500 to assess structured math-\\nematical reasoning. GSM-8K (Cobbe et al., 2021)\\ncontains 8,000 grade-school-level word problems,\\ntesting multi-step reasoning and numerical preci-\\nsion. MATH500 (Lightman et al., 2023) features\\n500 advanced problems from the MATH dataset\\n(Hendrycks et al., 2021), covering algebra, calcu-\\nlus, and geometry. It challenges models with tasks\\nrequiring symbolic manipulation and deep math-\\nematical understanding. These datasets help eval-\\nuate our approach across diverse scenarios, from\\nsimple arithmetic to complex problems.\\nLanguage Models.\\nThe schema design and table\\nconstruction modules in Table as Thought require\\nLLMs capable of generating complex, structured\\noutputs that conform to intricate schemas. This\\ncapability is natively supported by OpenAI’s Struc-\\ntured Outputs Mode, which allows for precise align-\\nment with defined schema requirements. Conse-\\nquently, our experiments are conducted exclusively\\non OpenAI’s GPT-4-o-mini and GPT-4-o-2024-08-\\n06 models (OpenAI et al., 2024). Expanding the\\nevaluation to include open-source models with sim-\\nilar capabilities remains an area for future work.\\n4.2\\nText Thought Baselines\\nDirect Prompting.\\nDirect Prompting involves\\nsolving queries by directly generating an answer\\nfrom the input, without prompting for any interme-\\ndiate reasoning steps.\\nCoT\\nPrompting.\\nChain-of-Thought\\n(CoT)\\nPrompting organizes reasoning as a sequential\\nchain of thoughts, thereby injecting structure into\\nthe reasoning process.\\nText as Thought.\\nThis approach differs from Ta-\\nble as Thought only in its use of unstructured repre-\\nsentations for thoughts. Text as Thought employs\\ntext as the medium for reasoning. This method\\nextends CoT prompting by iteratively updating the\\nreasoning process based on reflection. Each iter-\\nation involves generating intermediate reasoning\\nsteps, reflecting on their correctness, and refining\\nthe reasoning path as needed. The streamlined pro-\\ncess is formalized in Algorithm 2.\\nAlgorithm 2 Text as Thought\\nRequire: Query Q\\nEnsure: A text T that satisfies Q\\n1: Initialize an empty text T.\\n2: while not SUFFICIENT(T, Q) do\\n3:\\nΘ ←REFLECT(T, Q) // Generate possible\\nupdates\\n4:\\nT ←UPDATETEXT(T, Θ)\\n// Apply\\nupdates if needed\\n5: end while\\n6: return T\\n4.3\\nVariations of Table as Thought\\nTo fully explore and understand the boundaries of\\nTable as Thought, we introduce two variations to\\nthe TravelPlanner task. These variations include\\nTable as Thought with auto check constraint, which\\nadds complexity to schema design, and Table as\\nThought with given schema, which simplifies the\\ntask by providing a predefined schema.\\nTable as Thought with Auto-Check Constraint.\\nThis variation builds on the vanilla Table as\\nThought by requiring the LLM to add additional\\nconstraints during schema design to ensure data\\nintegrity and reflect the constraints present in the\\nquery. For instance, if a TravelPlanner query in-\\ncludes budget constraints, the LLM is expected to\\ndesign a schema with headers like Cost and en-\\nforce a rule ensuring that the sum of the column\\ndoes not exceed the specified budget. By introduc-\\ning this variation, we aim to explore the boundaries\\nof LLMs in designing complex reasoning structures\\nand handling intricate schema requirements.\\nTable as Thought with Given Schema.\\nIn this\\nvariation, the LLM is provided with a predefined\\nschema, as shown in Table 7, rather than designing\\nthe schema independently. The given schema is\\nderived from the evaluation pipeline of the Trav-\\nelPlanner task (Xie et al., 2024), where answers are\\nprocessed into tables following this schema before\\nevaluation. This variation serves as a comparative\\nbaseline to assess the effectiveness and adaptability\\nof schemas designed by LLMs compared to fixed,\\npredefined schemas.\\n5\\nResults\\n5.1\\nCalendar Scheduling Task\\nTable as Thought achieves the highest performance\\namong all prompting methods on the Calendar\\nScheduling Task, as shown in Table 2. On GPT-4o,\\nTable as Thought improves performance by 10.8%\\nover Direct Prompting and achieves a 5.4% im-\\nprovement compared to the Text as Thought base-\\nline. This highlights the advantage of using tables\\nas structured representations for planning over un-\\nstructured text-based representations. A similar\\ntrend is observed with GPT-4o-mini, where Table\\nas Thought outperforms other methods, suggesting\\nthe robustness of table-based reasoning for simpler\\nconstraint reasoning tasks like calendar scheduling.\\nFor GPT-4o, the improvement from Direct\\nPrompting to CoT Prompting is minimal (0.5%),\\nindicating that chain-like reasoning structures may\\nalready be embedded in the model’s reasoning\\nprocess. However, incorporating self-verification\\nthrough Text as Thought yields a 4.9% improve-\\nment. Importantly, transitioning from unstructured\\nthoughts to structured tables results in a more sub-\\nstantial performance boost (5.4%), underscoring\\nthe benefits of structured representations in reason-\\ning tasks.\\nMetric\\nDirect\\nCoT\\nText as Thought\\nTable as Thought\\nVanilla\\nw/ Auto-Check constraint\\nw/ Given Schema\\nDelivery Rate (%)\\n100.0\\n100.0\\n100.0\\n100.0\\n99.4\\n100.0\\nCommonsense Constraint Micro Pass Rate (%)\\n68.3\\n69.0\\n68.3\\n64.4\\n63.8\\n70.1\\nCommonsense Constraint Macro Pass Rate (%)\\n2.22\\n2.22\\n0.556\\n0.0\\n0.0\\n3.33\\nHard Constraint Micro Pass Rate (%)\\n7.62\\n6.19\\n3.81\\n3.33\\n1.90\\n5.95\\nHard Constraint Macro Pass Rate (%)\\n4.44\\n4.44\\n2.78\\n1.67\\n0.556\\n5.00\\nFinal Pass Rate (%)\\n0.556\\n0.556\\n0.0\\n0.0\\n0.0\\n1.11\\nTable 1: Evaluation results for different models and prompt methods on TraverPlanner Tasks on GPT4o-mini\\nDirect\\nCoT\\nText as Thought\\nTable as Thought\\nGPT-4o\\n64.0\\n64.5\\n69.4\\n74.8\\nGPT-4o-mini\\n36.2\\n38.4\\n38.4\\n42.3\\nTable 2: Performance of GPT-4o and GPT-4o-mini\\nmodels under different prompting methods for calendar\\nscheduling.\\nDirect\\nCoT\\nText as Thought\\nTable as Thought\\nMATH500\\nGPT-4o\\n75.0\\n72.2\\n72.6\\n64.2\\nGPT-4o-mini\\n65.4\\n65.2\\n63.4\\n47.8\\nGSM8K\\nGPT-4o\\n95.4\\n95.9\\n95.7\\n94.1\\nGPT-4o-mini\\n93.9\\n93.6\\n92.9\\n92.4\\nTable 3: Performance of GPT-4o and GPT-4o-mini mod-\\nels under different prompting methods for MATH500\\nand GSM8K.\\nDirect\\nCoT\\nText as Thought\\nMATH500\\nGPT-4o\\n4.4/25.0\\n5.4/27.8\\n4.4/27.4\\nGPT-4o-mini\\n2.0/36.6\\n2.4/34.6\\n2.8/34.8\\nGSM8K\\nGPT-4o\\n1.59/4.62\\n1.29/4.09\\n1.60/4.33\\nGPT-4o-mini\\n1.59/6.14\\n2.12/6.37\\n2.50/7.13\\nTable 4: The Percentage of Questions that Table as\\nThought successfully work out while other prompting\\nmethods failed vs failed rate of other prompting methods\\nFor GPT-4o-mini, a less advanced model, CoT\\nPrompting achieves a moderate 2.2% improvement\\nover Direct Prompting, but Text as Thought fails\\nto provide any additional gains. This suggests that\\nGPT-4o-mini lacks both the natural incorporation\\nof chain-like structures in its reasoning and the self-\\nverification capability to improve performance on\\ntext-based reasoning tasks. In contrast, Table as\\nThought demonstrates a significant 4.4% improve-\\nment over CoT Prompting, reinforcing the effec-\\ntiveness of introducing structure at the thought level\\nover chain-like structures at the reasoning level for\\nless advanced models.\\n5.2\\nTravelPlanner Task\\nTable 1 shows that Table as Thought with a given\\nschema achieves the best performance across most\\nmetrics in the TravelPlanner task, underscoring the\\npotential of structured thoughts. The significant\\nimprovement from vanilla Table as Thought to Ta-\\nble as Thought with a given schema highlights that\\ncurrent LLMs struggle to design effective table\\nschemas for achieving complex objectives. This\\nlimitation will be analyzed in more detail in the\\nnext section.\\nThe results reveal an important trend: on a chal-\\nlenging task like TravelPlanner, which demands\\nreasoning toward a complex objective, introduc-\\ning increasingly sophisticated structures into the\\nreasoning process can lead to performance degra-\\ndation. Specifically, methods that incorporate addi-\\ntional complexity—such as chain-of-thought (CoT)\\nprompting, self-reflection in Text as Thought, and\\nrule-constrained structured thoughts in Table as\\nThought with Auto-Check constraint—tend to per-\\nform worse compared to simpler approaches. The\\nexception is Table as Thought with a given schema,\\nwhich avoids this degradation by relieving the LLM\\nof the need to design its own schema, allowing it\\nto focus solely on reasoning within a predefined\\nstructure.\\n5.3\\nMath Reasoning Tasks\\nTable 3 highlights a general trend in the MATH500\\nand GSM8K tasks: introducing additional complex-\\nity into the reasoning process often leads to a per-\\nformance drop, particularly for less capable models\\nlike GPT-4o-mini. For instance, on MATH500, the\\nperformance of both GPT-4o and GPT-4o-mini de-\\ncreases as the reasoning structures become more\\nsophisticated, from Direct Prompting to Text as\\nThought to Table as Thought. This effect is es-\\npecially pronounced for GPT-4o-mini, where the\\nperformance of Table as Thought falls to 47.8%,\\ncompared to 65.4% with Direct Prompting. A sim-\\nilar trend is observed on GSM8K, where the addi-\\ntion of more structured reasoning methods results\\nin marginal performance degradation. These re-\\nsults suggest that LLMs may already be overfitted\\nto math reasoning tasks, as noted in recent studies\\n(Mirzadeh et al., 2024; Zhang et al., 2024a).\\nDespite this general trend, Table as Thought\\ndemonstrates its potential to improve performance\\nby successfully solving questions that text-thought-\\nbased methods fail to address, particularly with\\nmore capable models like GPT-4o. Table 4 pro-\\nvides a detailed breakdown of the percentage of\\nquestions that Table as Thought solves, which were\\nmissed by other methods. On MATH500, Table\\nas Thought resolves approximately 20% of such\\nquestions, while on GSM8K, this figure exceeds\\n30%. These findings underscore the utility of struc-\\ntured reasoning in identifying alternative pathways\\nto solutions that text-based reasoning methods may\\noverlook.\\n6\\nAnalysis\\n6.1\\nEffect of Schema Design on Reasoning\\nStructures\\nSchema design plays a pivotal role in structuring\\nthe reasoning paths of Calendar Scheduling tasks.\\nDifferent schemas determine the granularity of the\\nreasoning process, which in turn affects model per-\\nformance.\\nTable 5 shows that in the one-row schema, the\\nreasoning process is concise: the LLM identifies all\\navailable time slots for participants in a single step\\nand selects a suitable meeting time. This schema\\nproduces a single-row table, encapsulating the rea-\\nsoning process in a compact form. In contrast, the\\nmulti-row schema divides the process into finer-\\ngrained steps. The LLM first extracts unavailable\\nand preferred time slots for each participant. It then\\ncomputes available time slots before aggregating\\nthis information to finalize the meeting time. This\\napproach results in a table with multiple rows, each\\nrepresenting an intermediate reasoning step, and\\nprovides a more detailed reasoning path.\\nTable 6 shows that schema complexity impacts\\nperformance differently for advanced and less capa-\\nble models. For GPT-4o, the multi-row schema out-\\nperforms the one-row schema, achieving 80.28%\\naccuracy compared to 72.93%.\\nThis suggests\\nthat the finer-grained reasoning path introduced by\\nthe multi-row schema aligns well with GPT-4o’s\\nstronger table reasoning capabilities. By explic-\\nitly structuring intermediate steps, the multi-row\\nschema allows GPT-4o to better manage constraints\\nand ensure reasoning correctness. On the contrary,\\nGPT-4o-mini performs better with the simpler one-\\nrow schema (45.05% vs. 43.46% for the multi-row\\nschema). This indicates that the increased complex-\\nity of the multi-row schema exceeds the model’s\\ntable reasoning and verification abilities, leading to\\nperformance degradation.\\nSchema Example\\nOne Row\\nTime Slot, Jesse Availability\\nKathryn Availability, Megan Availability\\nAll Participants Available, Earliest Availability\\nMulti Row\\nParticipant Name, Availability Start Time\\nAvailability End Time, Meeting Duration\\nWork Hours Constraint, Schedule Constraint\\nPreference Constraint, Proposed Meeting Time\\nTable 5: Schema examples for Multi Row Thought and\\nOne Row Thought\\nGPT-4o-mini\\nGPT-4o\\nOne Row\\n45.05\\n72.93\\nMulti Row\\n43.46\\n80.28\\nTable 6: Performance Comparison of Multi Row and\\nOne Row Schemas for GPT-4o-mini and GPT-4o on\\nCalendar Scheduling\\n6.2\\nLLM Struggles to Design Effective\\nSchema for Complex Planning\\nUnlike Calendar Scheduling, which focuses on se-\\nlecting a single time slot, TravelPlanner involves\\ngenerating a comprehensive travel itinerary, sig-\\nnificantly increasing the complexity of the plan-\\nning task. Our findings indicate that tasking the\\nLLM with designing a table schema results in a\\nnotable performance drop compared to using di-\\nrect prompting with a pre-defined schema. This\\nsuggests that LLMs currently lack the capability to\\nindependently design effective table schemas for\\ncomplex planning tasks.\\nAlthough the provided schema is not per-\\nfect—omitting some critical columns, such as\\n\"cost\" for budget constraints—it is generally more\\neffective than most LLM-designed schemas. For\\ninstance, as shown in Table 7, the LLM-developed\\nschema and the given schema are structurally simi-\\nlar. However, a key difference is the use of \"Dining\\nOptions\" in the LLM-designed schema, as opposed\\nto separating dining into specific categories like\\n\"breakfast,\" \"lunch,\" and \"dinner.\" In practice, this\\nsimplification often leads the LLM to allocate only\\na single meal per day, which contradicts common-\\nsense expectations for travel planning.\\nSchema Example\\nGiven Schema\\ndays, current_city, attraction,\\ntransportation, breakfast,\\nlunch, dinner, accommodation\\nLLM Developed Schema\\nDay, Date, Location,\\nTransportation Details,\\nAccommodation Details,\\nActivities/Attractions, Dining Options,\\nEstimated Cost, Notes/Preferences\\nTable 7: Given Schema and Example of GPT-4o devel-\\noped Schema\\n6.3\\nAblation Study\\nSchema Designing\\nResoning Verification\\nACC(%)\\n✓\\n✓\\n42.3\\n✓\\n×\\n38.5 (↓3.8)\\n×\\n✓\\n36.2 (↓6.1)\\n×\\n×\\n32.7 (↓9.6)\\nTable 8: Ablation study results for GPT-4o-mini with\\nschema designing and reasoning verification effects on\\nperformance of calendar scheduling.\\nWe conducted an ablation study using GPT-4o-\\nmini on the Calendar Scheduling task to evaluate\\nthe individual contributions of schema design and\\nreasoning verification . Table 8 shows that when\\nreasoning verification is removed, accuracy drops\\nfrom 42.3% to 38.5% (↓3.8%). This indicates that\\nwithout explicitly verifying constraints, the LLM\\nmay overlook key restrictions in the query, leading\\nto false positives during self-checking. The absence\\nof schema design leads to a larger performance\\ndrop, from 42.3% to 36.2% (↓6.1%), and further\\nto 32.7% (↓9.6%) when both schema design and\\nreasoning verification are removed. This highlights\\nthe critical role of schema design in structuring\\nthe reasoning process. Table 9 shows that with-\\nout a schema, the LLM tends to create tables with\\nfewer columns, omitting key information neces-\\nsary for constraint checking. While the table with-\\nout schema design contains basic headers such as\\nParticipant and Selected Meeting Time, the\\nschema-designed table includes additional headers\\nlike Conflict Check, Work Hours Start/End,\\nand Notes/Comments. These additional columns\\ncapture critical reasoning steps and constraints, en-\\nabling more effective verification and selection of\\na valid meeting time.\\nColumn Headers\\nwo/ Schema Design\\nParticipant, Available Time Slots,\\nSelected Meeting Time\\nw/ Schema Design\\nParticipant Name, Participant Availability,\\nMeeting Duration, Meeting Day,\\nProposed Meeting Time, Work Hours Start,\\nWork Hours End, Conflict Check,\\nFinal Meeting Time , Notes/Comments\\nTable 9: Example of Column Headers of Table Thoughts\\nw/wo Schema Design\\n7\\nConclusion\\nWe proposed Table as Thought, a novel framework\\nthat introduces structured reasoning at the thought\\nlevel. The framework centers on the design and uti-\\nlization of table schemas, where the LLM is tasked\\nwith constructing a schema and generating struc-\\ntured thoughts based on it. Our results demonstrate\\nthat Table as Thought excels in constraint planning\\ntasks, showcasing its ability to manage complex\\nconstraints effectively. Moreover, the framework\\nexhibits significant potential for further improv-\\ning performance in math reasoning tasks, partic-\\nularly in addressing unsolved problems through\\nstructured reasoning.\\nAdditionally, we conducted detailed analyses of\\nthe results, exploring the interplay between schema\\ndesign, reasoning complexity, and model capabil-\\nities. These insights pave the way for future re-\\nsearch into the nature and representation of thought\\nprocesses, offering a promising direction for the\\ndevelopment of more robust reasoning frameworks\\nin LLMs.\\nLimitations\\nOur proposed methods are currently supported only\\nby models capable of generating structured data\\nwith complex schemas. This limitation restricts our\\nexperiments to a small set of closed-source models,\\nsuch as those provided by OpenAI. Consequently,\\nthe generalizability of our findings to open-source\\nLLMs remains unexplored. Future work should in-\\nvestigate approaches for adapting Table as Thought\\nto a broader range of models, including those with\\nlimited native support for structured data genera-\\ntion.\\nEthical Statement\\nThis research was conducted using publicly avail-\\nable datasets (e.g., GSM-8K, MATH500, Trav-\\nelPlanner) in compliance with their terms of use,\\nensuring no personally identifiable information\\n(PII) was processed. While our proposed frame-\\nwork, Table as Thought, aims to enhance structured\\nreasoning in LLMs, we acknowledge the potential\\nrisks of misuse in harmful applications, such as\\ndeceptive planning or adversarial reasoning. To\\nmitigate this, we advocate for responsible deploy-\\nment with appropriate safeguards.\\nReferences\\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\\nLehmann, Michał Podstawski, Hubert Niewiadomski,\\nPiotr Nyczyk, and Torsten Hoefler. 2024. Graph of\\nThoughts: Solving Elaborate Problems with Large\\nLanguage Models. Proceedings of the AAAI Confer-\\nence on Artificial Intelligence, 38(16):17682–17690.\\nKalina Christoff and John D. E. Gabrieli. 2000. The\\nfrontopolar cortex and human cognition: Evidence\\nfor a rostrocaudal hierarchical organization within\\nthe human prefrontal cortex. Psychobiology, 28:168–\\n186.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\\nNakano, Christopher Hesse, and John Schulman.\\n2021. Training verifiers to solve math word prob-\\nlems. arXiv preprint arXiv:2110.14168.\\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\\nacterizing embeddings of relational tables. Preprint,\\narXiv:2310.07736.\\nNaihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yu-\\nlong Chen, Lin Ma, Yue Zhang, and Rada Mihalcea.\\n2024. Tables as texts or images: Evaluating the table\\nreasoning ability of LLMs and MLLMs. In Find-\\nings of the Association for Computational Linguis-\\ntics: ACL 2024, pages 407–426, Bangkok, Thailand.\\nAssociation for Computational Linguistics.\\nKarl Friston. 2005.\\nA theory of cortical re-\\nsponses. Philosophical Transactions of the Royal\\nSociety of London. Series B, Biological Sciences,\\n360(1456):815–836.\\nJeff Hawkins. 2021. A Thousand Brains: A New Theory\\nof Intelligence, first edition edition. Basic Books,\\nHachette Book Group, Inc., New York, NY.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\nArora, Steven Basart, Eric Tang, Dawn Song, and\\nJacob Steinhardt. 2021. Measuring mathematical\\nproblem solving with the math dataset. Preprint,\\narXiv:2103.03874.\\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\\nMüller, Francesco Piccinno, and Julian Eisenschlos.\\n2020. TaPas: Weakly supervised table parsing via\\npre-training. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 4320–4333, Online. Association for Computa-\\ntional Linguistics.\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\\nEdwards, Bowen Baker, Teddy Lee, Jan Leike,\\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\\n2023.\\nLet’s verify step by step.\\nPreprint,\\narXiv:2305.20050.\\nIman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi,\\nOncel Tuzel, Samy Bengio, and Mehrdad Farajtabar.\\n2024. Gsm-symbolic: Understanding the limitations\\nof mathematical reasoning in large language models.\\nPreprint, arXiv:2410.05229.\\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\\ning Bao, Mohammad Bavarian, Jeff Belgum, Ir-\\nwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko,\\nMadelaine Boyd, Anna-Luisa Brakman, Greg Brock-\\nman, Tim Brooks, Miles Brundage, Kevin Button,\\nTrevor Cai, Rosie Campbell, Andrew Cann, Brittany\\nCarey, Chelsea Carlson, Rory Carmichael, Brooke\\nChan, Che Chang, Fotis Chantzis, Derek Chen, Sully\\nChen, Ruby Chen, Jason Chen, Mark Chen, Ben\\nChess, Chester Cho, Casey Chu, Hyung Won Chung,\\nDave Cummings, Jeremiah Currier, Yunxing Dai,\\nCory Decareaux, Thomas Degry, Noah Deutsch,\\nDamien Deville, Arka Dhar, David Dohan, Steve\\nDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,\\nTyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\\nSimón Posada Fishman, Juston Forte, Isabella Ful-\\nford, Leo Gao, Elie Georges, Christian Gibson, Vik\\nGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-\\nLopes, Jonathan Gordon, Morgan Grafstein, Scott\\nGray, Ryan Greene, Joshua Gross, Shixiang Shane\\nGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\\nYuchen He, Mike Heaton, Johannes Heidecke, Chris\\nHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin\\nHu, Joost Huizinga, Shantanu Jain, Shawn Jain,\\nJoanne Jang, Angela Jiang, Roger Jiang, Haozhun\\nJin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-\\nwoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-\\nmali, Ingmar Kanitscheider, Nitish Shirish Keskar,\\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim,\\nChristina Kim, Yongjik Kim, Jan Hendrik Kirch-\\nner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,\\nŁukasz Kondraciuk, Andrew Kondrich, Aris Kon-\\nstantinidis, Kyle Kosic, Gretchen Krueger, Vishal\\nKuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan\\nLeike, Jade Leung, Daniel Levy, Chak Ming Li,\\nRachel Lim, Molly Lin, Stephanie Lin, Mateusz\\nLitwin, Theresa Lopez, Ryan Lowe, Patricia Lue,\\nAnna Makanju, Kim Malfacini, Sam Manning, Todor\\nMarkov, Yaniv Markovski, Bianca Martin, Katie\\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer\\nMcKinney, Christine McLeavey, Paul McMillan,\\nJake McNeil, David Medina, Aalok Mehta, Jacob\\nMenick, Luke Metz, Andrey Mishchenko, Pamela\\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\\nMossing, Tong Mu, Mira Murati, Oleg Murk, David\\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\\nArvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\\nLong Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex\\nPaino, Joe Palermo, Ashley Pantuliano, Giambat-\\ntista Parascandolo, Joel Parish, Emy Parparita, Alex\\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perel-\\nman, Filipe de Avila Belbute Peres, Michael Petrov,\\nHenrique Ponde de Oliveira Pinto, Michael, Poko-\\nrny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-\\nell, Alethea Power, Boris Power, Elizabeth Proehl,\\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\\nCameron Raymond, Francis Real, Kendra Rimbach,\\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ry-\\nder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\\nGirish Sastry, Heather Schmidt, David Schnurr, John\\nSchulman, Daniel Selsam, Kyla Sheppard, Toki\\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav\\nShyam, Szymon Sidor, Eric Sigler, Maddie Simens,\\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\\nSokolowsky, Yang Song, Natalie Staudacher, Fe-\\nlipe Petroski Such, Natalie Summers, Ilya Sutskever,\\nJie Tang, Nikolas Tezak, Madeleine B. Thompson,\\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng,\\nPreston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\\nlipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\\nChelsea Voss, Carroll Wainwright, Justin Jay Wang,\\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\\nCJ Weinmann, Akila Welihinda, Peter Welinder, Ji-\\nayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\\nClemens Winter, Samuel Wolrich, Hannah Wong,\\nLauren Workman, Sherwin Wu, Jeff Wu, Michael\\nWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-\\ning Yuan, Wojciech Zaremba, Rowan Zellers, Chong\\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao\\nZheng, Juntang Zhuang, William Zhuk, and Bar-\\nret Zoph. 2024. Gpt-4 technical report. Preprint,\\narXiv:2303.08774.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\\nDenny Zhou. 2023. Self-consistency improves chain\\nof thought reasoning in language models. Preprint,\\narXiv:2203.11171.\\nZilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin\\nEisenschlos, Vincent Perot, Zifeng Wang, Lesly Mi-\\nculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee,\\nand Tomas Pfister. 2024. Chain-of-table: Evolving\\ntables in the reasoning chain for table understanding.\\nPreprint, arXiv:2401.04398.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\\nDenny Zhou. 2023. Chain-of-thought prompting elic-\\nits reasoning in large language models. Preprint,\\narXiv:2201.11903.\\nJian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze\\nLou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024.\\nTravelplanner: A benchmark for real-world planning\\nwith language agents. Preprint, arXiv:2402.01622.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L. Griffiths,\\nYuan Cao,\\nand Karthik\\nNarasimhan. 2023.\\nTree of thoughts:\\nDeliber-\\nate problem solving with large language models.\\nPreprint, arXiv:2305.10601.\\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\\nbastian Riedel. 2020. TaBERT: Pretraining for joint\\nunderstanding of textual and tabular data. In Proceed-\\nings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics, pages 8413–8426, On-\\nline. Association for Computational Linguistics.\\nHugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson,\\nCatherine Wu, Will Song, Tiffany Zhao, Pranav\\nRaja, Charlotte Zhuang, Dylan Slack, Qin Lyu, Sean\\nHendryx, Russell Kaplan, Michele Lunati, and Sum-\\nmer Yue. 2024a. A careful examination of large lan-\\nguage model performance on grade school arithmetic.\\nPreprint, arXiv:2405.00332.\\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\\n2024b. Tablellama: Towards open large generalist\\nmodels for tables. Preprint, arXiv:2311.09206.\\nHuaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang,\\nXinyun Chen, Minmin Chen, Azade Nova, Le Hou,\\nHeng-Tze Cheng, Quoc V. Le, Ed H. Chi, and\\nDenny Zhou. 2024.\\nNatural plan: Benchmark-\\ning llms on natural language planning.\\nPreprint,\\narXiv:2406.04520.\\nA\\nCompletion Rate Analysis of Table as\\nThought on Closed and Open Source\\nLLMs\\nWe investigate the completion rate of Table as\\nThought when using both closed-source and open-\\nsource LLMs in calendar scheduling tasks. De-\\nspite advancements in open-source models such as\\nLLaMA 3.1-8B and LLaMA 3.1-70B, these mod-\\nels encounter significant challenges in adhering to\\nthe tool schema required for the schema design and\\ntable construction modules. Specifically, they fail\\nto generate the expected outputs due to the com-\\nplexity of the tool schema, leading to errors in the\\nintermediate pipeline.\\nIn a set of 100 test examples, we observe that\\nTable as Thought fails 100% and 63% of the time\\nwhen using LLaMA 3.1-8B and LLaMA 3.1-70B,\\nrespectively. These high failure rates make experi-\\nments with open-source models infeasible because:\\n1. High Computational Cost: The need for re-\\npeated retries significantly increases computa-\\ntional overhead.\\n2. Result Bias: Retrying may introduce bias,\\nas incorrect reasoning processes may be ver-\\nified as correct, prematurely terminating the\\npipeline and artificially increasing the success\\nrate.\\nIn contrast, closed-source models such as GPT-\\n4 variants demonstrate much higher success rates.\\nThe results are summarized in Table 10.\\nTable 10: Success Rates of Table as Thought with Dif-\\nferent LLMs on Calendar Scheduling Tasks\\nModel\\nCompletion Rate (%)\\nLLaMA 3.1-8B\\n0\\nLLaMA 3.1-70B\\n37\\nGPT-4o\\n100\\nGPT-4o-mini\\n99\\nB\\nExample of Table as Thought on TraverPlanner Task\\nB.1\\nTable as Thought w/ Given Schema\\nQuery: Please draw up a 3-day travel itinerary for one person, beginning in Oakland and heading\\nto Tucson from March 15th to March 17th, 2022, with a budget of $1,400.\\nRaesoning table:\\n1\\n\"day1\": {\\n2\\n\"days\": 1,\\n3\\n\"current_city \": \"Oakland\",\\n4\\n\"transportation \": \"Flight Number: F4002752 , from Oakland to Tucson ,\\nDeparture Time: 15:07, Arrival Time: 17:00\" ,\\n5\\n\"breakfast \": \"-\",\\n6\\n\"attraction \": \"-\",\\n7\\n\"lunch\": \"-\",\\n8\\n\"dinner \": \"Pizza Street , Tucson\",\\n9\\n\"accommodation \": \"Private room with private bathroom , Tucson\",\\n10\\n\"total_cost \": 270,\\n11\\n\"id\": \"day1\"\\n12\\n},\\n13\\n\"day2\": {\\n14\\n\"days\": 2,\\n15\\n\"current_city \": \"Tucson\",\\n16\\n\"transportation \": \"-\",\\n17\\n\"breakfast \": \"Mocha , Tucson\",\\n18\\n\"attraction \": \"Pima Air & Space Museum , Tucson\",\\n19\\n\"lunch\": \"Pizza Street , Tucson\",\\n20\\n\"dinner \": \"Canteen Till I Die , Tucson\",\\n21\\n\"accommodation \": \"Room for rent shared bathroom , Tucson\",\\n22\\n\"total_cost \": 61,\\n23\\n\"id\": \"day2\"\\n24\\n},\\n25\\n\"day3\": {\\n26\\n\"days\": 3,\\n27\\n\"current_city \": \"Tucson\",\\n28\\n\"transportation \": \"Self -driving from Tucson to Oakland , Duration: 12 hours\\n42 mins , Cost: $68\",\\n29\\n\"breakfast \": \"-\",\\n30\\n\"attraction \": \"-\",\\n31\\n\"lunch\": \"-\",\\n32\\n\"dinner \": \"-\",\\n33\\n\"accommodation \": \"-\",\\n34\\n\"total_cost \": 68,\\n35\\n\"id\": \"day3\"\\n36\\n}\\nB.2\\nTable as Thought w/ Self-designed Schema\\nQuery: Please draw up a 3-day travel itinerary for one person, beginning in Oakland and heading\\nto Tucson from March 15th to March 17th, 2022, with a budget of $1,400.\\nRaesoning table:\\n1\\n\"1\": {\\n2\\n\"Day\": \"Day 1\",\\n3\\n\"Date\": \"2022 -03 -15\" ,\\n4\\n\"Travel Mode\": \"Flight\",\\n5\\n\"Departure Location \": \"Oakland\",\\n6\\n\"Destination \": \"Tucson\",\\n7\\n\"Travel Duration \": \"1 hour 53 minutes\",\\n8\\n\"Travel Cost\": 190,\\n9\\n\"Accommodation Name\": \"Private room with private bathroom\",\\n10\\n\"Accommodation Cost\": 58,\\n11\\n\"Room Type\": \"Private room\",\\n12\\n\"House Rules\": \"No smoking\",\\n13\\n\"Attractions \": \"Pima Air & Space Museum (Cost: $15), Reid Park Zoo (Cost:\\n$10)\",\\n14\\n\"Dining Options \": \"Villa Tevere (Cost: $37), Magic Spice Wok (Cost: $31)\",\\n15\\n\"Total Trip Cost\": 341,\\n16\\n\"Budget Remaining \": 1059,\\n17\\n\"id\": \"1\",\\n18\\n\"Maximum Occupancy \": \"2 people\"\\n19\\n},\\n20\\n\"2\": {\\n21\\n\"Day\": \"Day 2\",\\n22\\n\"Date\": \"2022 -03 -16\" ,\\n23\\n\"Travel Mode\": \"Self -driving\",\\n24\\n\"Departure Location \": \"Tucson\",\\n25\\n\"Destination \": \"Oakland\",\\n26\\n\"Travel Duration \": \"12 hours 40 minutes\",\\n27\\n\"Travel Cost\": 68,\\n28\\n\"Accommodation Name\": \"Private room with private bathroom\",\\n29\\n\"Accommodation Cost\": 58,\\n30\\n\"Room Type\": \"Private room\",\\n31\\n\"House Rules\": \"No smoking\",\\n32\\n\"Attractions \": \"Tucson Botanical Gardens (Cost: $15), Old Tucson (Cost:\\n$20)\",\\n33\\n\"Dining Options \": \"La Plage (Cost: $93), Ooh Lala! (Cost: $70)\",\\n34\\n\"Total Trip Cost\": 324,\\n35\\n\"Budget Remaining \": 735,\\n36\\n\"id\": \"2\",\\n37\\n\"Maximum Occupancy \": \"2 people\"\\n38\\n},\\n39\\n\"3\": {\\n40\\n\"Day\": \"Day 3\",\\n41\\n\"Date\": \"2022 -03 -17\" ,\\n42\\n\"Travel Mode\": \"Self -driving\",\\n43\\n\"Departure Location \": \"Tucson\",\\n44\\n\"Destination \": \"Oakland\",\\n45\\n\"Travel Duration \": \"12 hours 42 minutes\",\\n46\\n\"Travel Cost\": 68,\\n47\\n\"Accommodation Name\": \"None\",\\n48\\n\"Accommodation Cost\": 0,\\n49\\n\"Room Type\": \"Private room\",\\n50\\n\"House Rules\": \"No smoking\",\\n51\\n\"Attractions \": \"Arizona -Sonora Desert Museum (Cost: $15), Children ’s\\nMuseum Tucson (Cost: $10)\",\\n52\\n\"Dining Options \": \"Pirates of Grill (Cost: $52), Mood 4 Food (Cost: $20)\",\\n53\\n\"Total Trip Cost\": 165,\\n54\\n\"Budget Remaining \": 570,\\n55\\n\"id\": \"3\",\\n56\\n\"Maximum Occupancy \": \"N/A\"\\n57\\n}\\nC\\nExample of Table as Thought on Calendar Scheduling Task\\nC.1\\nTable as Thought w/ One Row Schema\\nQuery: You need to schedule a meeting for Roy, Kathryn and Amy for half an hour between the\\nwork hours of 9:00 to 17:00 on Monday.\\nHere are the existing schedules for everyone during the day: Roy has blocked their calendar on\\nMonday during 9:00 to 9:30, 10:00 to 10:30, 11:00 to 11:30, 12:30 to 13:00; Kathryn has meetings\\non Monday during 9:30 to 10:00, 16:30 to 17:00; Amy has blocked their calendar on Monday\\nduring 9:00 to 14:30, 15:00 to 16:00, 16:30 to 17:00;\\nAmy would rather not meet on Monday after 15:30. Find a time that works for everyone’s schedule\\nand constraints.\\nRaesoning table:\\n1\\n\"1\": {\\n2\\n\"Meeting Duration \": \"30 minutes\",\\n3\\n\"Work Hours Start\": \"9:00\" ,\\n4\\n\"Work Hours End\": \"17:00\" ,\\n5\\n\"Roy ’s Unavailable Times\": \"9:00 -9:30 , 10:00 -10:30 , 11:00 -11:30 ,\\n12:30 -13:00\" ,\\n6\\n\"Kathryn ’s Unavailable Times \": \"9:30 -10:00 , 16:30 -17:00\" ,\\n7\\n\"Amy ’s Unavailable Times\": \"9:00 -14:30 , 15:00 -16:00 , 16:30 -17:00\" ,\\n8\\n\"Amy ’s Preference \": \"Prefers not to meet after 15:30\" ,\\n9\\n\"Proposed Meeting Time\": \"14:30 -15:00\"\\n10\\n}\\nC.2\\nTable as Thought w/ Multi Row Schema\\nQuery: You need to schedule a meeting for Kathryn, Charlotte and Lauren for half an hour\\nbetween the work hours of 9:00 to 17:00 on Monday.\\nHere are the existing schedules for everyone during the day: Kathryn has blocked their calendar\\non Monday during 9:00 to 9:30, 10:30 to 11:00, 11:30 to 12:00, 13:30 to 14:30, 16:30 to 17:00;\\nCharlotte has blocked their calendar on Monday during 12:00 to 12:30, 16:00 to 16:30; Lauren has\\nblocked their calendar on Monday during 9:00 to 10:00, 12:00 to 12:30, 13:30 to 14:30, 15:00 to\\n16:00, 16:30 to 17:00;\\nCharlotte do not want to meet on Monday after 13:30. Find a time that works for everyone’s\\nschedule and constraints.\\nRaesoning table:\\n1\\n\"1\": {\\n2\\n\"Participant Name\": \"Kathryn\",\\n3\\n\"Unavailable Time Slots\": \"9:00 -9:30 , 10:30 -11:00 , 11:30 -12:00 ,\\n13:30 -14:30 , 16:30 -17:00\" ,\\n4\\n\"Preferred Meeting Time\": \"None\",\\n5\\n\"Meeting Duration \": \"30 minutes\",\\n6\\n\"Work Hours\": \"9:00 -17:00\" ,\\n7\\n\"Available Time Slots\": \"9:30 -10:30 , 11:00 -11:30 , 12:00 -13:30 ,\\n14:30 -16:30\" ,\\n8\\n\"Constraints Satisfied \": true ,\\n9\\n\"Proposed Meeting Time\": \"10:00 -10:30\"\\n10\\n},\\n11\\n\"2\": {\\n12\\n\"Participant Name\": \"Charlotte\",\\n13\\n\"Unavailable Time Slots\": \"12:00 -12:30 , 16:00 -16:30\" ,\\n14\\n\"Preferred Meeting Time\": \"Before 13:30\" ,\\n15\\n\"Meeting Duration \": \"30 minutes\",\\n16\\n\"Work Hours\": \"9:00 -17:00\" ,\\n17\\n\"Available Time Slots\": \"9:00 -12:00 , 12:30 -16:00 , 16:30 -17:00\" ,\\n18\\n\"Constraints Satisfied \": true ,\\n19\\n\"Proposed Meeting Time\": \"10:00 -10:30\"\\n20\\n},\\n21\\n\"3\": {\\n22\\n\"Participant Name\": \"Lauren\",\\n23\\n\"Unavailable Time Slots\": \"9:00 -10:00 , 12:00 -12:30 , 13:30 -14:30 ,\\n15:00 -16:00 , 16:30 -17:00\" ,\\n24\\n\"Preferred Meeting Time\": \"None\",\\n25\\n\"Meeting Duration \": \"30 minutes\",\\n26\\n\"Work Hours\": \"9:00 -17:00\" ,\\n27\\n\"Available Time Slots\": \"10:00 -12:00 , 12:30 -13:30 , 14:30 -15:00\" ,\\n28\\n\"Constraints Satisfied \": true ,\\n29\\n\"Proposed Meeting Time\": \"10:00 -10:30\"\\n30\\n}\\nD\\nComparison of Table as Thought and Direct Prompting on GSM-8k\\nD.1\\nExample where Direct Prompting succeceds while Table as Thought Fails\\nQuery: Stephen placed an online order for groceries. His final bill came to $40.00. Because this\\nwas through a delivery vendor, they tacked on a 25% fee to his final total and charged him $3.00\\nin delivery fees. Stephen also added a $4.00 tip. After the extra fees, what was the final price of\\nStephen’s groceries?\\nRaesoning table:\\n1\\n\"1\": {\\n2\\n\"Base_Price \": 40.0,\\n3\\n\"Delivery_Fee_Percentage \": 0,\\n4\\n\"Delivery_Fee_Amount \": 0,\\n5\\n\"Tip_Amount \": 0,\\n6\\n\"Total_Fee_Percentage \": 0,\\n7\\n\"Total_Fee_Amount \": 0,\\n8\\n\"Final_Price \": 40.0\\n9\\n}\\nDirect Prompting response:\\nTo find the final price of Stephen’s groceries after all the extra fees, we need to calculate each\\ncomponent and add them together.\\n1. Original Bill: Stephen’s original grocery bill was $40.00.\\n2. Vendor Fee: The vendor adds a 25% fee to the original bill. To calculate this fee:\\nVendor Fee = 0.25 × 40.00 = 10.00\\n3. Delivery Fee: The delivery fee is a flat $3.00.\\n4. Tip: Stephen added a $4.00 tip.\\nNow, we add all these amounts together to find the final price:\\nFinal Price = Original Bill + Vendor Fee + Delivery Fee + Tip\\nFinal Price = 40.00 + 10.00 + 3.00 + 4.00 = 57.00\\nTherefore, the final price of Stephen’s groceries, after all the extra fees, is $57.00.\\nD.2\\nExample where Table as Thought succeceds while Direct Prompting Fails\\nQuery: Jamal’s phone can hold 6 times more photographs than can Brittany’s phone. The\\nmaximum number of photographs that Brittany’s phone can hold is 50 times more than the number\\nof birds in Jamal’s photograph of the ducks at the zoo. If Jamal’s phone can hold 1800 photographs,\\nhow many ducks can be seen in Jamal’s photograph of ducks at the zoo?\\nRaesoning table:\\n1\\n\"1\": {\\n2\\n\"Jamal_Phone_Capacity \": 1800,\\n3\\n\"Brittany_Phone_Capacity \": 300,\\n4\\n\"Ducks_in_Photograph \": 6,\\n5\\n\"Brittany_Phone_Additional_Capacity \": 50,\\n6\\n\"Equation \": \"300 = 50D\",\\n7\\n\"Solution_Steps \": \"1. Jamal ’s phone can hold 6 times more photographs than\\nBrittany ’s phone. 2. Jamal ’s phone can hold 1800 photographs. 3. Let\\nB be the number of photographs Brittany ’s phone can hold. Then , 6B =\\n1800. 4. Solve for B: B = 1800 / 6 = 300. 5. Brittany ’s phone can hold\\n50 times more photographs than the number of ducks in Jamal ’s\\nphotograph. 6. Let D be the number of ducks in Jamal ’s photograph. B =\\n50D. 7. Substitute B = 300 into the equation: 300 = 50D. 8. Solve for\\nD: D = 300 / 50 = 6.\",\\n8\\n\"Final_Answer \": 6\\n9\\n}\\nDirect Prompting response:\\nTo solve the problem, we need to determine the number of ducks in Jamal’s photograph.\\nFirst, let’s define the variables:\\n• Let B be the maximum number of photographs Brittany’s phone can hold.\\n• Let D be the number of ducks in Jamal’s photograph.\\nAccording to the problem, Jamal’s phone can hold 6 times more photographs than Brittany’s phone.\\nTherefore, we have the equation:\\nJ = 6B\\nwhere J is the number of photographs Jamal’s phone can hold. We know from the problem that\\nJ = 1800. Substituting this into the equation gives:\\n1800 = 6B\\nTo find B, divide both sides by 6:\\nB = 1800\\n6\\n= 300\\nNext, the problem states that Brittany’s phone can hold 50 more photographs than the number of\\nducks in Jamal’s photograph. This gives us the equation:\\nB = D + 50\\nWe already found that B = 300. Substituting this into the equation gives:\\n300 = D + 50\\nTo find D, subtract 50 from both sides:\\nD = 300 −50 = 250\\nThus, the number of ducks in Jamal’s photograph is 250 .\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = search_arxiv(query=\"Chain of Thought\", max_docs=10)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SerpQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "class SerpQuery(BaseModel):\n",
    "\n",
    "    query: str\n",
    "    research_goal: str\n",
    "\n",
    "\n",
    "class SerpQueryResponse(BaseModel):\n",
    "    queries: List[SerpQuery] = Field(\n",
    "        ..., description=\"A list of SERP queries to be used for research\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_serp_queries(\n",
    "    query: str,\n",
    "    model_name: str,\n",
    "    num_queries: int = 3,\n",
    "    learnings: Optional[List[str]] = None,\n",
    ") -> List[SerpQuery]:\n",
    "    \"\"\"\n",
    "    사용자의 쿼리와 이전 연구 결과를 바탕으로 SERP 검색 쿼리를 생성합니다.\n",
    "    json_parser를 사용해 구조화된 JSON을 반환\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\n",
    "    ### System:\n",
    "    {system_prompt}\n",
    "    \n",
    "    ### Instructions:\n",
    "    다음 사용자의 입력을 기반으로 연구 주제를 조사하기 위한 SERP 검색 쿼리를 생성하세요.\n",
    "    JSON 객체를 반환하며, 'queries' 배열 필드에 {num_queries}개의 쿼리를 포함합니다.(쿼리가 명확할 경우 더 적을 수도 있음)\n",
    "    각 쿼리 객체에는 'query'와 'research_goal' 필드가 포함되어야 하며, 각 쿼리는 고유해야 합니다.\n",
    "    \n",
    "    ### Input:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    if learnings:  # 이전 연구에서 얻은 학습 내용이 있는 경우\n",
    "        prompt += f\"\\n\\n다음은 이전 연구에서 얻은 학습 내용입니다. 이를 활용하여 더 구체적인 쿼리를 생성하세요: {' '.join(learnings)}\"\n",
    "\n",
    "    prompt += \"\\n\\n### Answer:\\n\\n{format_instructions}\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(prompt)\n",
    "\n",
    "    json_parser = JsonOutputParser(pydantic_object=SerpQueryResponse)\n",
    "    sys_prompt = system_prompt()\n",
    "\n",
    "    prompt = prompt.partial(system_prompt=sys_prompt,\n",
    "                            format_instructions=json_parser.get_format_instructions())\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=model_name)\n",
    "\n",
    "    chain = prompt | llm | json_parser\n",
    "    response_json = chain.invoke({\"query\": query, \"num_queries\": num_queries})\n",
    "\n",
    "    try:\n",
    "        result = SerpQueryResponse.model_validate(response_json)\n",
    "        queries = result.queries if result.queries else []\n",
    "        print(f\"리서치 주제에 대한 SERP 검색 쿼리 {len(queries)}개 생성 완료\")\n",
    "        return queries[:num_queries]\n",
    "    except Exception as e:\n",
    "        print(f\"오류: generate_serp_queries에서 JSON 응답을 처리하는 중 오류 발생: {e}\")\n",
    "        print(f\"원시 응답: {response_json}\")\n",
    "        print(f\"오류: 쿼리 '{query}'에 대한 JSON 응답 처리 실패\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리서치 주제에 대한 SERP 검색 쿼리 3개 생성 완료\n"
     ]
    }
   ],
   "source": [
    "queries = generate_serp_queries(\n",
    "    query=\"Chain of Thought의 최신 연구 동향\", model_name=\"gemini-1.5-flash-8b\", num_queries=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = queries[0].query\n",
    "\n",
    "loder = ArxivLoader(\n",
    "    query=sample_query,\n",
    "    load_max_docs=10,\n",
    "    load_all_available=True\n",
    ")\n",
    "search_result = loder.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(docs[0].page_content)\n",
    "\n",
    "# from markdownify import markdownify as md\n",
    "\n",
    "# print(md(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchResult(BaseModel):\n",
    "    learnings: List[str]\n",
    "    visited_urls: List[str]\n",
    "\n",
    "\n",
    "class SerpResultResponse(BaseModel):\n",
    "    learnings: List[str] = Field(description=\"검색 결과로부터 추출된 주요 학습 내용 목록\")\n",
    "    followUpQuestions: List[str] = Field(\n",
    "        description=\"검색 결과를 바탕으로 생성된 후속 질문 목록\")\n",
    "\n",
    "\n",
    "def process_serp_result(\n",
    "    query: str,\n",
    "    search_result: List[SearchResult],\n",
    "    model_name: str,\n",
    "    num_learnings: int = 5,\n",
    "    num_follow_up_questions: int = 3,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    검색 결과를 처리하여 학습 내용과 후속 질문을 추출합니다.\n",
    "    json_parser를 사용해 구조화된 JSON을 반환합니다.\n",
    "    \"\"\"\n",
    "    contents = [item.page_content for item in search_result]\n",
    "\n",
    "    json_parser = JsonOutputParser(pydantic_object=SerpResultResponse)\n",
    "    contents_str = \"\".join(f\"<내용>{content}</내용>\" for content in contents)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"num_learnings\", \"num_follow_up_questions\"],\n",
    "        template=\"\"\"\n",
    "        ### System:\n",
    "        {system_prompt}\n",
    "        \n",
    "        ### Instruction:\n",
    "        다음은 쿼리 <쿼리>{query}</쿼리>에 대한 SERP검색 결과입니다.\n",
    "        이 내용을 바탕으로 학습 내용을 추출하고 후속 질문을 생성하세요.\n",
    "        JSON 객체로 반환하며, 'learnings' 및 'followUpQuestions' 키를 포함한 배열을 반환하요.\n",
    "        각 학습 내용은 고유하고 간결하며 정보가 풍부해야 합니다. 최대 {num_learnings}개의 학습 내용과\n",
    "        {num_follow_up_questions}개의 후속 질문을 포함해야 합니다.\\n\\n\n",
    "        <검색결과>{contents_str}</검색결과>\n",
    "        \n",
    "        ### Answer:\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "    ).partial(\n",
    "        system_prompt=system_prompt(),\n",
    "        contents_str=contents_str,\n",
    "        format_instructions=json_parser.get_format_instructions()\n",
    "    )\n",
    "    llm = ChatGoogleGenerativeAI(model=model_name)\n",
    "    chain = prompt | llm | json_parser\n",
    "    response_json = chain.invoke({\n",
    "        \"query\": query,\n",
    "        \"num_learnings\": num_learnings,\n",
    "        \"num_follow_up_questions\": num_follow_up_questions\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        result = SerpResultResponse.model_validate(response_json)\n",
    "        return {\n",
    "            \"learnings\": result.learnings,\n",
    "            \"followUpQuestions\": result.followUpQuestions\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"오류: process_serp_result에서 JSON 응답을 처리하는 중 오류 발생: {e}\")\n",
    "        print(f\"원시 응답: {response_json}\")\n",
    "        return {\n",
    "            \"learnings\": [],\n",
    "            \"followUpQuestions\": []\n",
    "        }\n",
    "\n",
    "\n",
    "def deep_research(\n",
    "    query: str,\n",
    "    breadth: int,\n",
    "    depth: int,\n",
    "    model_name: str,\n",
    "    learnings: Optional[List[str]] = None,\n",
    "    visited_papers: Optional[List[str]] = None,\n",
    ") -> ResearchResult:\n",
    "    \"\"\"\n",
    "    주제를 재귀적으로 탐색하여 SERP 쿼리를 생성하고, 검색 결과를 처리하며,\n",
    "    학습 내용과 방문한 URL을 수집합니다.\n",
    "    \"\"\"\n",
    "    learnings = learnings or []\n",
    "    visited_papers = visited_papers or []\n",
    "\n",
    "    print(f\"================Deep Research Start================\\n\")\n",
    "    print(f\"<주제> \\n {query} \\n <주제>\")\n",
    "\n",
    "    serp_queries = generate_serp_queries(\n",
    "        query=query,\n",
    "        model_name=model_name,\n",
    "        num_queries=breadth,\n",
    "        learnings=learnings\n",
    "    )\n",
    "    print(f\"================SERP 쿼리 생성 완료================\\n\")\n",
    "    print(f\"{serp_queries}\")\n",
    "\n",
    "    for index, serp_query in enumerate(serp_queries, start=1):\n",
    "        result: List[SearchResult] = search_arxiv(\n",
    "            query=serp_query.query, max_docs=depth,)\n",
    "        new_papers = [item.title for item in result if item.title]\n",
    "        serp_result = process_serp_result(\n",
    "            query=serp_query.query,\n",
    "            search_result=result,\n",
    "            model_name=model_name,\n",
    "            num_learnings=5,\n",
    "            num_follow_up_questions=breadth\n",
    "        )\n",
    "\n",
    "        all_learnings = learnings + serp_result['learnings']\n",
    "        all_papers = new_papers + visited_papers\n",
    "        new_depth = depth - 1\n",
    "        new_breadth = max(1, breadth // 2)\n",
    "\n",
    "        if new_depth > 0:\n",
    "            nex_query = (\n",
    "                f\"이전 연구목표: {serp_query.research_goal}\\n\"\n",
    "                f\"후속 연구방향: {\" \".join(serp_result['followUpQuestions'])}\"\n",
    "            )\n",
    "\n",
    "            # 증가된 시도 획수로 재귀 호출\n",
    "            sub_result = deep_research(\n",
    "                query=nex_query,\n",
    "                breadth=new_breadth,\n",
    "                depth=new_depth,\n",
    "                model_name=model_name,\n",
    "                learnings=all_learnings,\n",
    "                visited_papers=all_papers,\n",
    "            )\n",
    "\n",
    "            learnings = sub_result['learnings']\n",
    "            visited_papers = sub_result['visited_papers']\n",
    "\n",
    "        else:\n",
    "            learnings = all_learnings\n",
    "            visited_papers = all_papers\n",
    "\n",
    "        return {\n",
    "            \"learnings\": list(set(learnings)),\n",
    "            \"visited_papers\": list(set(visited_papers))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = process_serp_result(\n",
    "#     query=sample_query,\n",
    "#     search_result=search_result,\n",
    "#     model_name=\"gemini-1.5-flash-8b\",\n",
    "#     num_learnings=5,\n",
    "#     num_follow_up_questions=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result['learnings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================Deep Research Start================\n",
      "\n",
      "<주제> \n",
      " RAG 연구 동향 \n",
      " <주제>\n",
      "리서치 주제에 대한 SERP 검색 쿼리 3개 생성 완료\n",
      "================SERP 쿼리 생성 완료================\n",
      "\n",
      "[SerpQuery(query='Recent advancements in Retrieval-Augmented Generation (RAG) models', research_goal='Identify the latest architectural innovations, training methodologies, and performance benchmarks for RAG models.'), SerpQuery(query='Challenges and limitations of current RAG implementations', research_goal='Investigate the common problems encountered when deploying RAG systems, such as knowledge staleness, hallucination, computational cost, and sensitivity to prompt engineering.'), SerpQuery(query='Applications of Retrieval-Augmented Generation (RAG) across different industries', research_goal='Explore diverse use cases of RAG in fields like healthcare, finance, education, and customer service, focusing on how RAG is tailored to specific domain requirements and data types.')]\n",
      "================Deep Research Start================\n",
      "\n",
      "<주제> \n",
      " 이전 연구목표: Identify the latest architectural innovations, training methodologies, and performance benchmarks for RAG models.\n",
      "후속 연구방향: PORAG에서 검색 충실도와 응답 품질을 평가하는 데 사용되는 특정 보상 헤드는 무엇이며, 이러한 보상 간의 균형을 조정하는 데 사용되는 가중치 방식은 무엇인가요? ATLAS에서 MLAG 메커니즘이 정보 요구 사항을 감지하는 데 사용되는 다층 주의 기울기(MLAG)를 계산하는 데 사용되는 수학적 공식은 무엇인가요? CRITIC에서 토큰 중요도를 결정하기 위해 주의 기반, 엔트로피 기반 및 기울기 기반 전략이 결합되는 방식과 각 전략의 가중치를 동적으로 조정할 수 있는 방법은 무엇인가요? \n",
      " <주제>\n",
      "리서치 주제에 대한 SERP 검색 쿼리 6개 생성 완료\n",
      "================SERP 쿼리 생성 완료================\n",
      "\n",
      "[SerpQuery(query='PORAG \"dual reward mechanism\" \"group relative policy optimization\" retrieval fidelity response quality weights', research_goal='Identify the specific reward heads used in PORAG to evaluate retrieval fidelity and response quality, and investigate the weighting schemes used to balance these rewards. Determine how GRPO influences this balance.')]\n",
      "================Deep Research Start================\n",
      "\n",
      "<주제> \n",
      " 이전 연구목표: Identify the specific reward heads used in PORAG to evaluate retrieval fidelity and response quality, and investigate the weighting schemes used to balance these rewards. Determine how GRPO influences this balance.\n",
      "후속 연구방향: PORAG 및 ATLAS 프레임워크의 다양한 구성 요소가 검색 정확도 및 생성 품질에 기여하는 상대적 중요성은 무엇이며, 이러한 구성 요소는 서로 어떻게 상호 작용하여 전반적인 시스템 성능을 향상시키는가? \n",
      " <주제>\n",
      "리서치 주제에 대한 SERP 검색 쿼리 1개 생성 완료\n",
      "================SERP 쿼리 생성 완료================\n",
      "\n",
      "[SerpQuery(query='PORAG ATLAS CRITIC framework component contribution analysis', research_goal='Determine the relative contributions of PORAG, ATLAS, and CRITIC components to retrieval accuracy and generation quality. Analyze how these components interact to optimize overall system performance.')]\n"
     ]
    }
   ],
   "source": [
    "query = \"RAG 연구 동향\"\n",
    "breadth = 3\n",
    "depth = 3\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "result = deep_research(query, breadth, depth, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adaptive Token-Layer Attention Scoring for Selective Retrieval (ATLAS)는 Multi-Layer Attention Gradient (MLAG)를 사용하여 모델 레이어 간의 주의 패턴 변화를 분석하여 정보 요구 사항을 효율적으로 결정합니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['learnings'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refslave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
