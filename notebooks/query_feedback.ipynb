{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 후속 질문 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateQuestions(BaseModel):\n",
    "    questions: List[str] = Field(description=\"The answer to the user's question\")\n",
    "    \n",
    "def generate_feedback(query: str, model_name: str, max_feedbacks: int = 3) -> List[str]:\n",
    "    feedback_llm = ChatGoogleGenerativeAI(model=model_name)\n",
    "    \n",
    "    feedback_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Given the following query from the user, ask some follow up questions to clarify the research direction. \n",
    "        Return a maximum of ${max_feedbacks} questions, but feel free to return less if the original query is clear.\n",
    "        ask the follow up questions in korean\n",
    "        <query>${query}</query>`\n",
    "        # Answer:\\n\\n\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "    )\n",
    "    json_parser = JsonOutputParser(pydantic_object=GenerateQuestions)\n",
    "    \n",
    "    feedback_prompt = feedback_prompt.partial(format_instructions=json_parser.get_format_instructions())\n",
    "    \n",
    "    feedback_chain = feedback_prompt | feedback_llm | json_parser\n",
    "    \n",
    "    follow_up_questions = feedback_chain.invoke({\"query\": query, \"max_feedbacks\": max_feedbacks})\n",
    "    \n",
    "    try:\n",
    "        if follow_up_questions is None:\n",
    "            print(\"후속 질문이 생성되지 않았습니다.\")\n",
    "            return []\n",
    "        questions = follow_up_questions[\"questions\"]\n",
    "        print(f\"주제 {query}에 대한 후속 질문을 {max_feedbacks}개 생성했습니다.\")\n",
    "        print(f\"생성된 후속 질문: {questions}\")\n",
    "        return questions\n",
    "    except Exception as e:\n",
    "        print(f\"오류: JSON 응답 처리 중 문제 발생: {e}\")\n",
    "        print(f\"원시 응답: {follow_up_questions}\")\n",
    "        print(f\"오류: 쿼리 '{query}'에 대한 JSON 응답 처리 실패\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주제 RAG의 최근 동향을 알려줘.에 대한 후속 질문을 3개 생성했습니다.\n",
      "생성된 후속 질문: ['RAG의 어떤 측면에 대해서 알고 싶으신가요? (예: 기술적 동향, 시장 점유율, 투자 동향 등)', '특정 기간(예: 지난 6개월, 최근 1년)에 대한 정보가 필요하신가요?', 'RAG에 대한 정보를 어떻게 활용하실 예정인가요?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RAG의 어떤 측면에 대해서 알고 싶으신가요? (예: 기술적 동향, 시장 점유율, 투자 동향 등)',\n",
       " '특정 기간(예: 지난 6개월, 최근 1년)에 대한 정보가 필요하신가요?',\n",
       " 'RAG에 대한 정보를 어떻게 활용하실 예정인가요?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follow_up_questions = generate_feedback(query=\"RAG의 최근 동향을 알려줘.\", model_name=\"gemini-1.5-flash-8b\", max_feedbacks=3)\n",
    "follow_up_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refslave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
